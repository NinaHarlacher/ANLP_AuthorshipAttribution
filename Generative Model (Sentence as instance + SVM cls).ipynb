{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model\n",
    "\n",
    "In the following the approach of using a multi support vector machine classifier together with each sentence of a play as an instance will be explored. This is done to increase the number of training data passed to the classifier. The features implemented will be based on those used by Fox et al. (2012), which are: \n",
    "-\tFrequency of stop words\n",
    "-\tFrequency of POS tags of words which are not stop words\n",
    "-   Frequency of bigrams of stop words and POS tags \n",
    "\n",
    "In the end, each sentence will be assigned to one author, to determine the writer of an entire play the majority class of all sentence of one play will be taken.  \n",
    "To evaluate the accuracy, a three-fold cross validation will be employed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:32:07.899705Z",
     "start_time": "2019-02-28T15:31:42.051718Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "We prepare the data such that each sentence of a play is an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:32:07.914321Z",
     "start_time": "2019-02-28T15:32:07.903593Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus= glob(\"El/*\")\n",
    "corpus_tagged = glob('Tagged_or_Stopwords/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:32:07.921312Z",
     "start_time": "2019-02-28T15:32:07.916394Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ordered_lists(auhtors, corpus):\n",
    "    \"\"\" \n",
    "    input: corpus as a list with path of files and a list of authors\n",
    "    \n",
    "    output: list of list, with the same order of the author list, with a list containing all plays for each author\n",
    "    \"\"\"\n",
    "    dramas_per_author=[]\n",
    "    for author in auhtors:\n",
    "        authorList=[]\n",
    "        for drama in corpus:\n",
    "            if author in drama:\n",
    "                authorList.append(drama)\n",
    "        dramas_per_author.append(authorList)\n",
    "    \n",
    "    return dramas_per_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:32:07.934317Z",
     "start_time": "2019-02-28T15:32:07.925296Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lists(corpus, auhtors):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: list of authors, list of all files as paths for all plays\n",
    "    \n",
    "    output: list with  all files ordered according to author list \n",
    "    \"\"\"\n",
    "    corpus_per_author = get_ordered_lists(auhtors, corpus)\n",
    "    \n",
    "    data=[]\n",
    "    labels=[]\n",
    "    for idx, corpus in enumerate(corpus_per_author):\n",
    "        for idx2, play in enumerate(corpus):\n",
    "            instance= play\n",
    "            data.append(instance)\n",
    "\n",
    "            labels.append(idx)\n",
    "\n",
    "\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "y_authors = ['E-Shakespeare', 'L-Shakespeare', 'Marlowe', 'Middleton','Jonson', 'Chapman']\n",
    "\n",
    "data, labels = get_lists(corpus, y_authors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing stop word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:32:10.657303Z",
     "start_time": "2019-02-28T15:32:07.937291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "710\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "stopwrd1= []\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    stopwrd1.append(word)\n",
    "\n",
    "\n",
    "stopwords2 = stopwords.words('english')\n",
    "\n",
    "\n",
    "stopwords3 = get_stop_words('english')\n",
    "\n",
    "#from https://www.ranks.nl/stopwords\n",
    "stopwords4 = ['a ', 'able', 'about', 'above', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'again', 'against', 'ah', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apparently', 'approximately', 'are', 'aren', 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asking', 'at', 'auth', 'available', 'away', 'awfully', 'b', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'between', 'beyond', 'biol', 'both', 'brief', 'briefly', 'but', 'by', 'c', 'ca', 'came', 'can', 'cannot', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'co', 'com', 'come', 'comes', 'contain', 'containing', 'contains', 'could', 'couldnt', 'd', 'date', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'due', 'during', 'e', 'each', 'ed', 'edu', 'effect', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'especially', 'et', 'et-al', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'except', 'f', 'far', 'few', 'ff', 'fifth', 'first', 'five', 'fix', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'found', 'four', 'from', 'further', 'furthermore', 'g', 'gave', 'get', 'gets', 'getting', 'give', 'given', 'gives', 'giving', 'go', 'goes', 'gone', 'got', 'gotten', 'h', 'had', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', 'hed', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'hes', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'home', 'how', 'howbeit', 'however', 'hundred', 'i', 'id', 'ie', 'if', \"i'll\", 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inc', 'indeed', 'index', 'information', 'instead', 'into', 'invention', 'inward', 'is', \"isn't\", 'it', 'itd', \"it'll\", 'its', 'itself', \"i've\", 'j', 'just', 'k', 'keep\\tkeeps', 'kept', 'kg', 'km', 'know', 'known', 'knows', 'l', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'lets', 'like', 'liked', 'likely', 'line', 'little', \"'ll\", 'look', 'looking', 'looks', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'merely', 'mg', 'might', 'million', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'mug', 'must', 'my', 'myself', 'n', 'na', 'name', 'namely', 'nay', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'now', 'nowhere', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'ord', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'page', 'pages', 'part', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'put', 'q', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'run', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sent', 'seven', 'several', 'shall', 'she', 'shed', \"she'll\", 'shes', 'should', \"shouldn't\", 'show', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'since', 'six', 'slightly', 'so', 'some', 'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure\\tt', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \n",
    "             'thered', 'therefore', 'therein', \"there'll\", 'thereof', 'therere', 'theres', 'thereto', 'thereupon', \"there've\", 'these', 'they', 'theyd', \"they'll\", 'theyre', \"they've\", 'think', 'this', 'those', 'thou', 'though', 'though', 'thousand', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'tip', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'v', 'value', 'various', \"'ve\", 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'w', 'want', 'wants', 'was', 'wasnt', 'way', 'we', 'wed', 'welcome', \"we'll\", 'went', 'were', 'werent', \"we've\", 'what', 'whatever', \"what'll\", 'whats', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', \"who'll\", 'whom', 'whomever', 'whos', 'whose', 'why', 'widely', 'willing', 'wish', 'with', 'within', 'without', 'wont', 'words', 'world', 'would', 'wouldnt', 'www', 'x', 'y', 'yes', 'yet', 'you', 'youd', \"you'll\", 'your', 'youre', 'yours', 'yourself', 'yourselves', \"you've\", 'z', 'zero']\n",
    "\n",
    "        \n",
    "\n",
    "stopwords_305 = stopwrd1\n",
    "stopwords_747= list(set(stopwrd1 + stopwords2+ stopwords3 +stopwords4))\n",
    "stopwords_710= stopwords_747[:710]\n",
    "print(len(stopwords_305))\n",
    "print(len(stopwords_710))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframe with an instance being a sentence, each instance will have the following information:\n",
    "\n",
    "\n",
    "- All POS taggs if word is not a stopword for default stop words loaded with SpaCy and the additioanl passed one\n",
    "- POS taggs or stopwords for each stop word list\n",
    "- Lemmas for each word in stantance\n",
    "- Sentence Length \n",
    "- Label indicating author\n",
    "- Author name \n",
    "- Name of play\n",
    "- ID for each play    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:47:33.779230Z",
     "start_time": "2019-02-28T15:32:10.663333Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataframe(corpus, authors, stopwrds2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: corpus with all texts, authors, and additional list with stopwords\n",
    "    \n",
    "    runtime: 14m 27s for 76 files \n",
    "    \n",
    "    output: Pandas dataframe with one sentence is an instance, which has the following information:\n",
    "            - All POS taggs if word is not a stopword for default stop words loaded with SpaCy and the additional passed one\n",
    "            - POS taggs or stopwords for each stop word list\n",
    "            - Lemmas for each word in stantance\n",
    "            - Sentence Length \n",
    "            - Label indicating author\n",
    "            - Author name \n",
    "            - Name of play\n",
    "            - ID for each play    \n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en\")\n",
    "    data=[]   \n",
    "    corpus_per_author = get_ordered_lists(authors, corpus)\n",
    "    \n",
    "    play_id = -1\n",
    "    for idx, corpus in enumerate(corpus_per_author):\n",
    "        author=authors[idx]\n",
    "        label=idx\n",
    "\n",
    "        for play in corpus:\n",
    "            play_id += 1\n",
    "\n",
    "            drama=play[3:]\n",
    "            doc = nlp(open(play).read())\n",
    "            for sent in doc.sents:\n",
    "                sentlen= len(sent)\n",
    "                POS_stwrd = \" \"\n",
    "                sentence= \" \"\n",
    "                lemmas= \" \"\n",
    "                POSstopwrds2 = \" \"\n",
    "                POS= \" \"\n",
    "                POS2 = \" \"\n",
    "                for word in sent:\n",
    "                    sentence += str(word)\n",
    "                    sentence += str(\" \")\n",
    "                    lemmas += str(word.lemma_)\n",
    "                    lemmas += str(\" \")\n",
    "\n",
    "                    if word.is_stop:\n",
    "                        POS_stwrd += str(word) \n",
    "                        POS_stwrd += str(\" \") \n",
    "                        \n",
    "                    else:\n",
    "                        POS_stwrd  += str(word.pos_)\n",
    "                        POS_stwrd  += str(\" \")\n",
    "                       \n",
    "                    if str(word) in stopwrds2:\n",
    "                        POSstopwrds2 += str(word) \n",
    "                        POSstopwrds2 += str(\" \")\n",
    "                    else:\n",
    "                        POSstopwrds2  += str(word.pos_)\n",
    "                        POSstopwrds2  += str(\" \")\n",
    "                        \n",
    "                    if not word.is_stop:\n",
    "                        POS += str(word.pos_)\n",
    "                        POS += \" \"\n",
    "                    if str(word) not in stopwrds2:\n",
    "                        POS2 += str(word.pos_)\n",
    "                        POS2 += \" \"\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                data.append((sentence, POS, POS2, POS_stwrd, POSstopwrds2,  lemmas, sentlen, label, author, drama,  play_id))\n",
    "            \n",
    "    df=pd.DataFrame(data,\n",
    "                    columns=\n",
    "                    [\"Sentence\", \"POSnotstopwrds\", \"POSnotstopwrds2\", \"POSstopwrds\", \"POSstopwrds2\", \"Lemmas\", \"Sentencelen\",\"Label\", \"Author\", \"Play\", \"Play_id\" ] )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_authors = ['E-Shakespeare', 'L-Shakespeare', 'Marlowe', 'Middleton','Jonson', 'Chapman']\n",
    "df = create_dataframe(corpus, y_authors, stopwords_710)\n",
    "\n",
    "#saving df in directory as excel file called \"DataFrame\"\n",
    "df.to_excel(\"DataFrame_SVM.xlsx\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:16:21.299455Z",
     "start_time": "2019-02-28T16:15:51.404569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POSnotstopwrds</th>\n",
       "      <th>POSnotstopwrds2</th>\n",
       "      <th>POSstopwrds</th>\n",
       "      <th>POSstopwrds2</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Sentencelen</th>\n",
       "      <th>Label</th>\n",
       "      <th>Author</th>\n",
       "      <th>Play</th>\n",
       "      <th>Play_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As I remember , Adam , it was upon this fashi...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>as -PRON- remember , adam , -PRON- be upon th...</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother Jaques he keeps at school , and \\n...</td>\n",
       "      <td>ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...</td>\n",
       "      <td>ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...</td>\n",
       "      <td>ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...</td>\n",
       "      <td>ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...</td>\n",
       "      <td>-PRON- brother jaques -PRON- keep at school ,...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His horses \\n are bred better ; for , besides...</td>\n",
       "      <td>ADJ NOUN SPACE VERB ADV PUNCT PUNCT ADJ SPACE...</td>\n",
       "      <td>ADJ NOUN SPACE VERB VERB ADV PUNCT PUNCT VERB...</td>\n",
       "      <td>ADJ NOUN SPACE are VERB ADV PUNCT for PUNCT b...</td>\n",
       "      <td>ADJ NOUN SPACE VERB VERB ADV PUNCT for PUNCT ...</td>\n",
       "      <td>-PRON- horse \\n be breed better ; for , besid...</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Besides this nothing that he so \\n plentifull...</td>\n",
       "      <td>ADP SPACE ADV VERB PUNCT NOUN VERB SPACE NOUN...</td>\n",
       "      <td>ADP SPACE ADV PRON PUNCT NOUN SPACE PRON NOUN...</td>\n",
       "      <td>ADP this nothing that he so SPACE ADV VERB me...</td>\n",
       "      <td>ADP this nothing that he so SPACE ADV gives P...</td>\n",
       "      <td>besides this nothing that -PRON- so \\n plenti...</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is it , Adam , that \\n grieves me ; and ...</td>\n",
       "      <td>DET PUNCT PROPN PUNCT SPACE VERB PUNCT NOUN N...</td>\n",
       "      <td>DET PUNCT PROPN PUNCT SPACE VERB PRON PUNCT N...</td>\n",
       "      <td>DET is it PUNCT PROPN PUNCT that SPACE VERB m...</td>\n",
       "      <td>DET is it PUNCT PROPN PUNCT that SPACE VERB P...</td>\n",
       "      <td>this be -PRON- , adam , that \\n grieve -PRON-...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0   As I remember , Adam , it was upon this fashi...   \n",
       "1   My brother Jaques he keeps at school , and \\n...   \n",
       "2   His horses \\n are bred better ; for , besides...   \n",
       "3   Besides this nothing that he so \\n plentifull...   \n",
       "4   This is it , Adam , that \\n grieves me ; and ...   \n",
       "\n",
       "                                      POSnotstopwrds  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...   \n",
       "2   ADJ NOUN SPACE VERB ADV PUNCT PUNCT ADJ SPACE...   \n",
       "3   ADP SPACE ADV VERB PUNCT NOUN VERB SPACE NOUN...   \n",
       "4   DET PUNCT PROPN PUNCT SPACE VERB PUNCT NOUN N...   \n",
       "\n",
       "                                     POSnotstopwrds2  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...   \n",
       "2   ADJ NOUN SPACE VERB VERB ADV PUNCT PUNCT VERB...   \n",
       "3   ADP SPACE ADV PRON PUNCT NOUN SPACE PRON NOUN...   \n",
       "4   DET PUNCT PROPN PUNCT SPACE VERB PRON PUNCT N...   \n",
       "\n",
       "                                         POSstopwrds  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...   \n",
       "2   ADJ NOUN SPACE are VERB ADV PUNCT for PUNCT b...   \n",
       "3   ADP this nothing that he so SPACE ADV VERB me...   \n",
       "4   DET is it PUNCT PROPN PUNCT that SPACE VERB m...   \n",
       "\n",
       "                                        POSstopwrds2  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...   \n",
       "2   ADJ NOUN SPACE VERB VERB ADV PUNCT for PUNCT ...   \n",
       "3   ADP this nothing that he so SPACE ADV gives P...   \n",
       "4   DET is it PUNCT PROPN PUNCT that SPACE VERB P...   \n",
       "\n",
       "                                              Lemmas  Sentencelen  Label  \\\n",
       "0   as -PRON- remember , adam , -PRON- be upon th...           50      0   \n",
       "1   -PRON- brother jaques -PRON- keep at school ,...           68      0   \n",
       "2   -PRON- horse \\n be breed better ; for , besid...           66      0   \n",
       "3   besides this nothing that -PRON- so \\n plenti...           61      0   \n",
       "4   this be -PRON- , adam , that \\n grieve -PRON-...           55      0   \n",
       "\n",
       "          Author                               Play  Play_id  \n",
       "0  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0  \n",
       "1  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0  \n",
       "2  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0  \n",
       "3  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0  \n",
       "4  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read saved df\n",
    "df=pd.read_excel('DataFrame_SVM.xlsx', index_col=0) \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the same features as done by Fox et al. (2012)\n",
    "### using each sentence as instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:09:46.416107Z",
     "start_time": "2019-02-28T16:09:46.296867Z"
    }
   },
   "outputs": [],
   "source": [
    "#split the dataframe into train and test data\n",
    "train_data, test_data= train_test_split(df, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring which size of stop word sets has the better outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T15:06:57.415628Z",
     "start_time": "2019-02-22T15:06:56.607740Z"
    }
   },
   "source": [
    "#### 1. Using set of stop words of size 710 (as done by Fox and colleagues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:48:03.876701Z",
     "start_time": "2019-02-28T15:48:03.863706Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_sent = train_data[\"Sentence\"]\n",
    "X_train_POS_710 = train_data[\"POSnotstopwrds2\"]\n",
    "X_train_POSstp_710 = train_data[\"POSstopwrds2\"]\n",
    "\n",
    "y_train= train_data[\"Label\"]\n",
    "\n",
    "#do the same thing for test data\n",
    "X_test_sent = test_data[\"Sentence\"]\n",
    "X_test_POS_710 = test_data[\"POSnotstopwrds2\"]\n",
    "X_test_POSstp_710 = test_data[\"POSstopwrds2\"]\n",
    "\n",
    "y_test= test_data[\"Label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:48:18.310920Z",
     "start_time": "2019-02-28T15:48:03.879695Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cvec1 = CountVectorizer(vocabulary= stopwords_710, max_features=1000) \n",
    "cvec2 = CountVectorizer(max_features=1000)\n",
    "cvec3 = CountVectorizer(ngram_range=(2,2), max_features=1000)\n",
    "\n",
    "\n",
    "training_features_710 = np.hstack((\n",
    "    cvec1.fit_transform(X_train_sent).toarray(),\n",
    "    cvec2.fit_transform(X_train_POS_710).toarray(),\n",
    "    cvec3.fit_transform(X_train_POSstp_710).toarray(),\n",
    "    ))\n",
    "\n",
    "\n",
    "test_features_710 = np.hstack((\n",
    "        cvec1.transform(X_test_sent).toarray(),\n",
    "        cvec2.transform(X_test_POS_710).toarray(),\n",
    "        cvec3.transform(X_test_POSstp_710).toarray(),\n",
    "    ))\n",
    "\n",
    "\n",
    "# cvec1.vocabulary_\n",
    "# cvec2.vocabulary_\n",
    "# cvec3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:52:13.611003Z",
     "start_time": "2019-02-28T15:48:18.314907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninah\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(training_features_710,y_train )\n",
    "pred_svm=svm.score(test_features_710, y_test)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(training_features_710,y_train, y_train )\n",
    "pred_MB=nb.score(test_features_710, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:52:13.641159Z",
     "start_time": "2019-02-28T15:52:13.623994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of Naive Bayes has score of 0.3453691109187585\n",
      "Prediction of multi SVM has score of 0.46442850665677426\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction of Naive Bayes has score of \" + str(pred_MB))\n",
    "print(\"Prediction of multi SVM has score of \" + str(pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using smaller set of stopwords (305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:52:13.671772Z",
     "start_time": "2019-02-28T15:52:13.647756Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_sent = train_data[\"Sentence\"]\n",
    "X_train_POS_305 = train_data[\"POSnotstopwrds\"]\n",
    "X_train_POSstp_305 = train_data[\"POSstopwrds\"]\n",
    "\n",
    "y_train= train_data[\"Label\"]\n",
    "\n",
    "#do the same thing for test data\n",
    "X_test_sent= test_data[\"Sentence\"]\n",
    "X_test_POS_305= test_data[\"POSnotstopwrds\"]\n",
    "X_test_POSstp_305 = test_data[\"POSstopwrds\"]\n",
    "\n",
    "y_test= test_data[\"Label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:52:26.420087Z",
     "start_time": "2019-02-28T15:52:13.675371Z"
    }
   },
   "outputs": [],
   "source": [
    "#then define what columns I want\n",
    "\n",
    "cvec1 = CountVectorizer(vocabulary= stopwords_305, max_features=1000) \n",
    "cvec2 = CountVectorizer(max_features=1000)\n",
    "cvec3 = CountVectorizer(ngram_range=(2,2), max_features=1000)\n",
    "\n",
    "\n",
    "training_features_305 = np.hstack((\n",
    "    cvec1.fit_transform(X_train_sent).toarray(),\n",
    "    cvec2.fit_transform(X_train_POS_305).toarray(),\n",
    "    cvec3.fit_transform(X_train_POSstp_305).toarray(),\n",
    "    ))\n",
    "\n",
    "\n",
    "test_features_305 = np.hstack((\n",
    "        cvec1.transform(X_test_sent).toarray(),\n",
    "        cvec2.transform(X_test_POS_305).toarray(),\n",
    "        cvec3.transform(X_test_POS_305).toarray(),\n",
    "    ))\n",
    "# cvec1.vocabulary_\n",
    "# cvec2.vocabulary_\n",
    "# cvec3.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:56:21.517726Z",
     "start_time": "2019-02-28T15:52:26.425012Z"
    }
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(training_features_305, y_train)\n",
    "pred_305_svm= svm.score(test_features_305, y_test)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(training_features_305, y_train)\n",
    "pred_305_nb= nb.score(test_features_305, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T15:56:21.535734Z",
     "start_time": "2019-02-28T15:56:21.522458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of Naive Bayes has score of 0.39584106178640616\n",
      "Prediction of multi SVM has score of 0.4166357528543753\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction of Naive Bayes has score of \" + str(pred_305_nb))\n",
    "print(\"Prediction of multi SVM has score of \" + str(pred_305_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T21:38:46.046340Z",
     "start_time": "2019-02-19T21:38:26.979Z"
    }
   },
   "source": [
    "__Results__\n",
    "\n",
    "\n",
    "Overall, the results are low, nevertheless, the multi SVM  performs better across all set of stop words.  \n",
    "Moreover, the list with 710 stop words delivered a higher score.\n",
    "\n",
    "\n",
    "|     Stops  | NB    | SVM  |  \n",
    "|--------|--------|--------|\n",
    "| 305  | 0.3958 | 0.4166 |\n",
    "| 710 | 0.3454 | __0.4644__ |\n",
    "\n",
    "\n",
    "\n",
    "Thus, in the following I will use the bigger list of stop words with a multiclass SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for each play\n",
    "\n",
    "To get the prediction for each play, I will firstly use a 3-fold cross validation to get the prediction for each sentence.\n",
    "Following this I will look at each sentence of a play and take the majority class to get the final prediction of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:08:11.335210Z",
     "start_time": "2019-02-28T15:56:21.540985Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_sent = df[\"Sentence\"]\n",
    "X_POS_710 = df[\"POSnotstopwrds2\"] #do this with the bigger stop words set\n",
    "X_POSstp_710= df[\"POSstopwrds2\"]\n",
    "y = df[\"Label\"]\n",
    "\n",
    "cvec1 = CountVectorizer(vocabulary= stopwrd1, max_features=1000, strip_accents=\"ascii\") \n",
    "cvec2 = CountVectorizer(max_features=1000)\n",
    "cvec3 = CountVectorizer(ngram_range=(2,2), max_features=1000)\n",
    "\n",
    "\n",
    "X = np.hstack((\n",
    "    cvec1.fit_transform(X_sent).toarray(),\n",
    "    cvec2.fit_transform(X_POS_710).toarray(),\n",
    "    cvec3.fit_transform(X_POSstp_710).toarray(),\n",
    "    ))\n",
    "\n",
    "clf = LinearSVC()\n",
    "y_pred = cross_val_predict(clf, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:09:45.946212Z",
     "start_time": "2019-02-28T16:08:11.406434Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred\"] = y_pred\n",
    "df.head()\n",
    "df.to_excel(\"DataFrame_SVM_Preds.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:09:45.979054Z",
     "start_time": "2019-02-28T16:09:45.950232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POSnotstopwrds</th>\n",
       "      <th>POSnotstopwrds2</th>\n",
       "      <th>POSstopwrds</th>\n",
       "      <th>POSstopwrds2</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Sentencelen</th>\n",
       "      <th>Label</th>\n",
       "      <th>Author</th>\n",
       "      <th>Play</th>\n",
       "      <th>Play_id</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As I remember , Adam , it was upon this fashi...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>as -PRON- remember , adam , -PRON- be upon th...</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother Jaques he keeps at school , and \\n...</td>\n",
       "      <td>ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...</td>\n",
       "      <td>ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...</td>\n",
       "      <td>ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...</td>\n",
       "      <td>ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...</td>\n",
       "      <td>-PRON- brother jaques -PRON- keep at school ,...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His horses \\n are bred better ; for , besides...</td>\n",
       "      <td>ADJ NOUN SPACE VERB ADV PUNCT PUNCT ADJ SPACE...</td>\n",
       "      <td>ADJ NOUN SPACE VERB VERB ADV PUNCT PUNCT VERB...</td>\n",
       "      <td>ADJ NOUN SPACE are VERB ADV PUNCT for PUNCT b...</td>\n",
       "      <td>ADJ NOUN SPACE VERB VERB ADV PUNCT for PUNCT ...</td>\n",
       "      <td>-PRON- horse \\n be breed better ; for , besid...</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Besides this nothing that he so \\n plentifull...</td>\n",
       "      <td>ADP SPACE ADV VERB PUNCT NOUN VERB SPACE NOUN...</td>\n",
       "      <td>ADP SPACE ADV PRON PUNCT NOUN SPACE PRON NOUN...</td>\n",
       "      <td>ADP this nothing that he so SPACE ADV VERB me...</td>\n",
       "      <td>ADP this nothing that he so SPACE ADV gives P...</td>\n",
       "      <td>besides this nothing that -PRON- so \\n plenti...</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is it , Adam , that \\n grieves me ; and ...</td>\n",
       "      <td>DET PUNCT PROPN PUNCT SPACE VERB PUNCT NOUN N...</td>\n",
       "      <td>DET PUNCT PROPN PUNCT SPACE VERB PRON PUNCT N...</td>\n",
       "      <td>DET is it PUNCT PROPN PUNCT that SPACE VERB m...</td>\n",
       "      <td>DET is it PUNCT PROPN PUNCT that SPACE VERB P...</td>\n",
       "      <td>this be -PRON- , adam , that \\n grieve -PRON-...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0   As I remember , Adam , it was upon this fashi...   \n",
       "1   My brother Jaques he keeps at school , and \\n...   \n",
       "2   His horses \\n are bred better ; for , besides...   \n",
       "3   Besides this nothing that he so \\n plentifull...   \n",
       "4   This is it , Adam , that \\n grieves me ; and ...   \n",
       "\n",
       "                                      POSnotstopwrds  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...   \n",
       "2   ADJ NOUN SPACE VERB ADV PUNCT PUNCT ADJ SPACE...   \n",
       "3   ADP SPACE ADV VERB PUNCT NOUN VERB SPACE NOUN...   \n",
       "4   DET PUNCT PROPN PUNCT SPACE VERB PUNCT NOUN N...   \n",
       "\n",
       "                                     POSnotstopwrds2  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   ADJ NOUN PROPN VERB NOUN PUNCT SPACE NOUN VER...   \n",
       "2   ADJ NOUN SPACE VERB VERB ADV PUNCT PUNCT VERB...   \n",
       "3   ADP SPACE ADV PRON PUNCT NOUN SPACE PRON NOUN...   \n",
       "4   DET PUNCT PROPN PUNCT SPACE VERB PRON PUNCT N...   \n",
       "\n",
       "                                         POSstopwrds  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...   \n",
       "2   ADJ NOUN SPACE are VERB ADV PUNCT for PUNCT b...   \n",
       "3   ADP this nothing that he so SPACE ADV VERB me...   \n",
       "4   DET is it PUNCT PROPN PUNCT that SPACE VERB m...   \n",
       "\n",
       "                                        POSstopwrds2  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   ADJ NOUN PROPN he VERB at NOUN PUNCT and SPAC...   \n",
       "2   ADJ NOUN SPACE VERB VERB ADV PUNCT for PUNCT ...   \n",
       "3   ADP this nothing that he so SPACE ADV gives P...   \n",
       "4   DET is it PUNCT PROPN PUNCT that SPACE VERB P...   \n",
       "\n",
       "                                              Lemmas  Sentencelen  Label  \\\n",
       "0   as -PRON- remember , adam , -PRON- be upon th...           50      0   \n",
       "1   -PRON- brother jaques -PRON- keep at school ,...           68      0   \n",
       "2   -PRON- horse \\n be breed better ; for , besid...           66      0   \n",
       "3   besides this nothing that -PRON- so \\n plenti...           61      0   \n",
       "4   this be -PRON- , adam , that \\n grieve -PRON-...           55      0   \n",
       "\n",
       "          Author                               Play  Play_id  Pred  \n",
       "0  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0     0  \n",
       "1  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0     0  \n",
       "2  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0     1  \n",
       "3  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0     1  \n",
       "4  E-Shakespeare  asyoulikeit.txt.E-Shakespeare.tok        0     4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:10:42.766389Z",
     "start_time": "2019-02-28T16:10:42.532408Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_per_author, y = get_lists(corpus, y_authors)\n",
    "\n",
    "majority_class= {}\n",
    "for idx, play in enumerate(corpus_per_author):\n",
    "    mjc= mode(list(df.loc[lambda df: df['Play_id'] == idx][\"Pred\"]))[0]\n",
    "    majority_class[play]=mjc\n",
    "    \n",
    "    \n",
    "get_data= []\n",
    "\n",
    "for idx, play in enumerate(corpus_per_author):\n",
    "    canonical= y[idx]\n",
    "    pred= int(majority_class[play])\n",
    "    get_data.append((play, canonical, pred))\n",
    "    \n",
    "dfpred= pd.DataFrame(get_data, columns=[\"play\", \"canonical author\", \"predicted author\"])  \n",
    "# dfpred.head(77)\n",
    "dfpred[\"acc\"] =dfpred[\"canonical author\"]==dfpred[ \"predicted author\"]\n",
    "acc_total = dfpred.loc[dfpred.acc == True, 'acc'].count()/len(corpus_per_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T16:11:55.376104Z",
     "start_time": "2019-02-28T16:11:55.366706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is:  0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "print(\"The overall accuracy is: \", acc_total )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T19:58:58.515990Z",
     "start_time": "2019-02-22T19:58:58.487068Z"
    }
   },
   "source": [
    "__Results__\n",
    "\n",
    "The overall accuracy is very low with 54, 54%, and thus, this might be due the imbalanced data set. \n",
    "Thus, the approach with using a sentence as an instance will not be further explored, instead each play will be used as an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
