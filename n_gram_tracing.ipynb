{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram-Tracing (implemented after Grieve et al. 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of all text file names corresponding to one author \n",
    "\n",
    "e_shakespeare_texts = []\n",
    "l_shakespeare_texts = []\n",
    "marlowe_texts = []\n",
    "middleton_texts = []\n",
    "jonson_texts = []\n",
    "chapman_texts = []\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(\"EL\"):\n",
    "    file_count += 1\n",
    "    if filename.endswith(\"L-Shakespeare.tok\"): \n",
    "        l_shakespeare_texts.append(filename)\n",
    "    if filename.endswith(\"E-Shakespeare.tok\"): \n",
    "        e_shakespeare_texts.append(filename)\n",
    "    if filename.endswith(\"Marlowe.tok\"): \n",
    "        marlowe_texts.append(filename)\n",
    "    if filename.endswith(\"Middleton.tok\"): \n",
    "        middleton_texts.append(filename)        \n",
    "    if filename.endswith(\"Jonson.tok\"): \n",
    "        jonson_texts.append(filename)\n",
    "    if filename.endswith(\"Chapman.tok\"): \n",
    "        chapman_texts.append(filename)\n",
    "\n",
    "        \n",
    "all_author_files = [e_shakespeare_texts, l_shakespeare_texts,\n",
    "                   marlowe_texts, middleton_texts, jonson_texts, chapman_texts]\n",
    "all_author_names = [\"E-Shakespeare.tok\", \"L-Shakespeare.tok\", \"Marlowe.tok\", \"Middleton.tok\", \"Jonson.tok\", \"Chapman.tok\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_list(filelist, tested_file):\n",
    "    \"\"\"\n",
    "    Generates overlapping character ngram of a given order for a given document.\n",
    "    Input: List with document names, name of file to be tested\n",
    "    Output: List with characters\n",
    "    \"\"\"\n",
    "    \n",
    "    rawlist = []\n",
    "    \n",
    "    for document in filelist:\n",
    "        if document != tested_file:\n",
    "            with open(os.path.join(\"EL/\" + document), \"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in fh:\n",
    "                    raw_words = re.split(r'(\\s+)', line)\n",
    "                    for word in raw_words:\n",
    "                        stripped = word.lower()\n",
    "                        for character in stripped:\n",
    "                            rawlist.append(character)\n",
    "            \n",
    "    return rawlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_list(filelist, tested_file):\n",
    "    \"\"\"\n",
    "    Generates overlapping words ngram of a given order for a given document.\n",
    "    Input: List with document names, name of the file to be tested\n",
    "    Output: List with words\n",
    "    \"\"\"\n",
    "    \n",
    "    rawlist = []\n",
    "\n",
    "    for document in filelist:\n",
    "        if document != tested_file:\n",
    "            with open(os.path.join(\"EL/\" + document), \"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in fh:\n",
    "                    raw_words = line.split()\n",
    "                    for word in raw_words:\n",
    "                        stripped = word.lower()\n",
    "                        rawlist.append(stripped)\n",
    "            \n",
    "    return rawlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\".\"}\n",
    "\n",
    "def generate_ngram(rawlist, ngram_order):\n",
    "    ngrams = zip(*(rawlist[i:] for i in range(ngram_order)))\n",
    "    return (ngram for ngram in ngrams if not any(w in stopwords for w in ngram[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapp_coefficient(set_of_ngrams1,set_of_ngrams2):\n",
    "    \"\"\"\n",
    "    Input: Two sets (usually the ngrams of the file to be tested and of an author sample)\n",
    "    Output: Overlapp Co-Efficient as described by Grieve et al. 2017 \n",
    "    \"\"\"\n",
    "\n",
    "    return len(set(set_of_ngrams1).intersection(set(set_of_ngrams2))) / min(len(set(set_of_ngrams1)),len(set(set_of_ngrams2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_co_efficient(testfile, words_or_chars, ngram_order):\n",
    "    \"\"\"\n",
    "    Takes the length of the file to be tested and takes a random sample of lines from each play \n",
    "    of an author based on that, such that their sizes are equal to another.\n",
    "    Input: Name of the file to be tested, whether the co-efficient should be over words or\n",
    "        characters, ngram-order\n",
    "    Output: The co-efficient between the tested file and a randomized author sample\n",
    "    \"\"\"\n",
    "\n",
    "    punctuation = \"\"\" !\"',;:.-?)([]<>*#\\n\\t\\r \"\"\"\n",
    "    all_co_efficients = []\n",
    "    with open(os.path.join(\"EL/\" + testfile), \"r\", encoding=\"utf-8\") as fh:\n",
    "        length = len(fh.read())\n",
    "        \n",
    "        for author in all_author_files:\n",
    "            rawsamples = []\n",
    "            all_author_words = []\n",
    "            all_author_chars = []\n",
    "            list_of_tuples = []\n",
    "            chars_per_author_file = int(length / len(author))\n",
    "        \n",
    "            for file in author:\n",
    "                if file != testfile:\n",
    "                    with open(os.path.join(\"EL/\" + file), \"r\", encoding=\"utf-8\") as play:\n",
    "                        all_lines = []\n",
    "                        sum_of_chars = 0\n",
    "                \n",
    "                        for line in play:\n",
    "                            all_lines.append(str(line.strip(punctuation)))\n",
    "                            sum_of_chars += len(line)\n",
    "                    \n",
    "                        sum_of_lines = len(all_lines)\n",
    "                        line_length_average = int(sum_of_chars / sum_of_lines)\n",
    "                        amount_of_lines = int(chars_per_author_file / line_length_average)\n",
    "                        rawsamples += random.sample(all_lines, amount_of_lines)\n",
    "                \n",
    "                        for string in rawsamples:\n",
    "                            all_author_words += string.split()\n",
    "                            \n",
    "                        if words_or_chars == \"chars\":\n",
    "                            for word in all_author_words:\n",
    "                                all_author_chars += list(word)                       \n",
    "            \n",
    "            if words_or_chars == \"chars\":\n",
    "                set1 = {*generate_ngram(all_author_chars,ngram_order)}\n",
    "                set2 = {*generate_ngram(generate_char_list([testfile],\"empty\"),ngram_order)}\n",
    "            elif words_or_chars == \"words\":\n",
    "                set1 = {*generate_ngram(all_author_words,ngram_order)}\n",
    "                set2 = {*generate_ngram(generate_word_list([testfile],\"empty\"),ngram_order)}\n",
    "                \n",
    "            co_efficient = overlapp_coefficient(set1,set2)\n",
    "            all_co_efficients.append(co_efficient)\n",
    " \n",
    "    most_likely_author = all_author_names[all_co_efficients.index(max(all_co_efficients))]\n",
    "    return testfile, most_likely_author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_right = 0\n",
    "for play in os.listdir(\"EL\"):\n",
    "    testfile, most_likely_author = calculating_co_efficient(play,\"chars\",7)\n",
    "    if testfile.endswith(most_likely_author):\n",
    "        predicted_right += 1\n",
    "accuracy = predicted_right / file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for 7-character ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_right = 0\n",
    "for play in os.listdir(\"EL\"):\n",
    "    testfile, most_likely_author = calculating_co_efficient(play,\"chars\",8)\n",
    "    if testfile.endswith(most_likely_author):\n",
    "        predicted_right += 1\n",
    "accuracy = predicted_right / file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for 8-character ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6025641025641025\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_right = 0\n",
    "for play in os.listdir(\"EL\"):\n",
    "    testfile, most_likely_author = calculating_co_efficient(play,\"words\",2)\n",
    "    if testfile.endswith(most_likely_author):\n",
    "        predicted_right += 1\n",
    "accuracy = predicted_right / file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for 2-word ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2948717948717949\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the n-gram-tracing on \"Second Maiden's Tragedy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Maiden's Tragedy was written by  L-Shakespeare.tok  according to 7-character-ngram-tracing\n",
      "Second Maiden's Tragedy was written by  Middleton.tok  according to 8-character-ngram-tracing\n",
      "Second Maiden's Tragedy was written by  Middleton.tok  according to 2-word-ngram-tracing\n"
     ]
    }
   ],
   "source": [
    "testfile, most_likely_author = calculating_co_efficient(\"the second maiden.txt.Middleton.tok\",\"chars\",7)\n",
    "print(\"Second Maiden's Tragedy was written by \", most_likely_author,\" according to 7-character-ngram-tracing\")\n",
    "testfile, most_likely_author = calculating_co_efficient(\"the second maiden.txt.Middleton.tok\",\"chars\",8)\n",
    "print(\"Second Maiden's Tragedy was written by \", most_likely_author,\" according to 8-character-ngram-tracing\")\n",
    "testfile, most_likely_author = calculating_co_efficient(\"the second maiden.txt.Middleton.tok\",\"words\",2)\n",
    "print(\"Second Maiden's Tragedy was written by \", most_likely_author,\" according to 2-word-ngram-tracing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
