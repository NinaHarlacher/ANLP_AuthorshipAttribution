{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bag of words model for authorship attribution of Elizabethan plays is implemented after Fox et al. (2014), though with a Naive Bayes classifier. It only takes word frequencies into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages needed\n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "import os \n",
    "#change directory to EL folder with data\n",
    "os.chdir(\"EL\") \n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of author labels for prediction\n",
    "\n",
    "all_authors = ['Early Shakespeare', 'Late Shakespeare', 'Marlowe', 'Middleton',\n",
    "              'Jonson', 'Chapman']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a list of all text file names corresponding to one author \n",
    "\n",
    "\n",
    "e_shakespeare_texts = []\n",
    "l_shakespeare_texts = []\n",
    "marlowe_texts = []\n",
    "middleton_texts = []\n",
    "jonson_texts = []\n",
    "chapman_texts = []\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if filename.endswith(\"L-Shakespeare.tok\"): \n",
    "        l_shakespeare_texts.append(filename)\n",
    "    if filename.endswith(\"E-Shakespeare.tok\"): \n",
    "        e_shakespeare_texts.append(filename)\n",
    "    if filename.endswith(\"Marlowe.tok\"): \n",
    "        marlowe_texts.append(filename)\n",
    "    if filename.endswith(\"Middleton.tok\"): \n",
    "        middleton_texts.append(filename)        \n",
    "    if filename.endswith(\"Jonson.tok\"): \n",
    "        jonson_texts.append(filename)\n",
    "    if filename.endswith(\"Chapman.tok\"): \n",
    "        chapman_texts.append(filename)\n",
    "\n",
    "        \n",
    "all_author_files = [e_shakespeare_texts, l_shakespeare_texts,\n",
    "                   marlowe_texts, middleton_texts, jonson_texts, chapman_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_author(file_paths):\n",
    "    \"\"\"\n",
    "    Reads in texts of one author.\n",
    "    \n",
    "    Input: List of file names/paths of texts by same author\n",
    "    \n",
    "    Output: List of texts as strings\n",
    "    \"\"\"\n",
    "    author_corpus = []\n",
    "    for path in file_paths:\n",
    "        with open(path) as open_text:\n",
    "            text = open_text.read()\n",
    "        author_corpus.append(text)\n",
    "    return author_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in all files of each author\n",
    "\n",
    "e_shkp_data = read_files_author(e_shakespeare_texts)\n",
    "l_shkp_data = read_files_author(l_shakespeare_texts)\n",
    "marlowe_data = read_files_author(marlowe_texts)\n",
    "middleton_data = read_files_author(middleton_texts)\n",
    "jonson_data = read_files_author(jonson_texts)\n",
    "chapman_data = read_files_author(chapman_texts)\n",
    "\n",
    "all_author_texts = [e_shkp_data, l_shkp_data, marlowe_data,\n",
    "                   middleton_data, jonson_data, chapman_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute number of all texts used for classification\n",
    "\n",
    "number_texts = 0\n",
    "for author in all_author_texts:\n",
    "    number_texts += len(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_corpora(all_author_texts):\n",
    "    \"\"\"\n",
    "    Merges texts of one author into one document, for several authors given in list.\n",
    "    \n",
    "    Input: List of lists of texts per author\n",
    "    \n",
    "    Output: List of document per author as string\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for author_texts in all_author_texts:\n",
    "        text = ''\n",
    "        for author_text in author_texts:\n",
    "            text += author_text\n",
    "        corpus.append(text)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words as Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GV_feature_vector(corpus):\n",
    "    \"\"\"\n",
    "    Transforms a corpus into feature vectors representing counts of words.\n",
    "    \n",
    "    Input: List of documents making up the corpus\n",
    "    \n",
    "    Output: Array of word counts for each document (feature vectors)\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    word_count_matrix = vectorizer.fit_transform(corpus)\n",
    "    word_count_array = word_count_matrix.toarray()\n",
    "    \n",
    "    return word_count_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(word_count_array, all_authors):\n",
    "    \"\"\"\n",
    "    Builds a classifier from provided feature vectors.\n",
    "    \n",
    "    Input: 1) Array of feature vectors for each author and the test data\n",
    "    2) List of all authors considered, in same order as their feature vectors\n",
    "    \n",
    "    Output: Instance of a multinomial Naive Bayes classifier trained \n",
    "    with the data excluding the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_word_count = word_count_array[:-1]\n",
    "    \n",
    "    classifier = MultinomialNB(alpha = 0.02)\n",
    "    classifier.fit(train_word_count, all_authors)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bagofwords(corpus, all_authors):\n",
    "    \"\"\"\n",
    "    Pipeline putting above functions together.\n",
    "    \n",
    "    Input: 1) Corpus of all authors considered including the test text as last sample.\n",
    "    2) List of all authors considered, in same order as their feature vectors\n",
    "    \n",
    "    Output: Predicted class as string\n",
    "    \"\"\"\n",
    "    \n",
    "    word_count_array = GV_feature_vector(corpus)\n",
    "\n",
    "    test_word_count = word_count_array[-1]\n",
    "    test_word_count = test_word_count.reshape(1,-1)\n",
    "\n",
    "    cla = build_classifier(word_count_array, all_authors)\n",
    "\n",
    "    return cla.predict(test_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier with Leave One Out (LOO) Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is created. Accuracy: 0.8311688311688312\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run all cells above, then this one will provide leave one out cross validation.\n",
    "Creates a document with predictions and computed accuracy.\n",
    "\"\"\"\n",
    "#get out of EL folder\n",
    "new_dir = os.getcwd()[:-2]\n",
    "os.chdir(new_dir)\n",
    "\n",
    "leaveOO = LeaveOneOut()\n",
    "correct = 0 \n",
    "\n",
    "with open('BagOfWords_LOO', 'w') as bow_file:\n",
    "    \n",
    "    for author_position, author_texts in enumerate(all_author_texts):\n",
    "        \n",
    "        current_author = all_authors[author_position]\n",
    "        bow_file.write(current_author + '\\n')\n",
    "        \n",
    "        for train_index, test_index in leaveOO.split(author_texts):\n",
    "            train_author_texts = [author_texts[train] for train in train_index]\n",
    "            test_data = [author_texts[test] for test in test_index]\n",
    "            new_author_texts = [train_author_texts if texts == author_texts \n",
    "                                    else texts for texts in all_author_texts]\n",
    "\n",
    "            corpus = merge_corpora(new_author_texts)\n",
    "            corpus.extend(test_data)\n",
    "            \n",
    "            predicted_author = test_bagofwords(corpus, all_authors)[0]\n",
    "            \n",
    "            bow_file.write('\\t' + all_author_files[author_position][test_index[0]] + \n",
    "                           '\\n\\t' + predicted_author + '\\n\\n')\n",
    "            \n",
    "            if predicted_author == current_author:\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct/number_texts\n",
    "    bow_file.write('Accuracy: {}/{} = {}'.format(correct, number_texts, accuracy))\n",
    "\n",
    "print(\"File is created. Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
