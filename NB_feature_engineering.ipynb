{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Naive Bayes Feature Engineering \n",
    "\n",
    "Uses\n",
    "- Naive Bayes Classifier\n",
    "- provided data frame\n",
    "- basic features POS tag frequency, stop word frequency, n-grams of POS and stop words\n",
    "\n",
    "to test different additional features, namely\n",
    "- word frequency\n",
    "- keyword extraction\n",
    "- stemming\n",
    "- line length\n",
    "- sentence length\n",
    "- lemmatizing\n",
    "- hapax legomena (unique word frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE\n",
    "\n",
    "#Results can differ depending on how the CountVectorizers in the get_feature functions are built. Two versions \n",
    "#have been tried and been found to affect the results for better or for worse in a non consistent way \n",
    "#in each feature.\n",
    "\n",
    "#Version 1\n",
    "#cvec1 = CountVectorizer(vocabulary = stopwords_507, strip_accents=\"ascii\")#word freq of stop words\n",
    "#cvec2 = CountVectorizer() #freq of POS tags\n",
    "#cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops\n",
    "\n",
    "#Version 2\n",
    "#cvec1 = CountVectorizer(vocabulary= stopwords_507, strip_accents=\"ascii\")#word freq of stop words\n",
    "#cvec2 = CountVectorizer(max_features=1000, strip_accents=\"ascii\") #freq of POS tags\n",
    "#cvec3 = CountVectorizer(ngram_range=(2,2),max_features=1000, strip_accents=\"ascii\") #ngrams of POS/stops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:51:45.224984Z",
     "start_time": "2019-02-24T15:51:45.217826Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T12:58:51.992795Z",
     "start_time": "2019-02-23T12:58:51.011699Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "stopwrd1= []\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    stopwrd1.append(word)\n",
    "\n",
    "\n",
    "stopwords2 = stopwords.words('english')\n",
    "\n",
    "\n",
    "stopwords3 = get_stop_words('english')\n",
    "\n",
    "#from https://www.ranks.nl/stopwords\n",
    "stopwords4 = ['a ', 'able', 'about', 'above', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'again', 'against', 'ah', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apparently', 'approximately', 'are', 'aren', 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asking', 'at', 'auth', 'available', 'away', 'awfully', 'b', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'between', 'beyond', 'biol', 'both', 'brief', 'briefly', 'but', 'by', 'c', 'ca', 'came', 'can', 'cannot', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'co', 'com', 'come', 'comes', 'contain', 'containing', 'contains', 'could', 'couldnt', 'd', 'date', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'due', 'during', 'e', 'each', 'ed', 'edu', 'effect', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'especially', 'et', 'et-al', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'except', 'f', 'far', 'few', 'ff', 'fifth', 'first', 'five', 'fix', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'found', 'four', 'from', 'further', 'furthermore', 'g', 'gave', 'get', 'gets', 'getting', 'give', 'given', 'gives', 'giving', 'go', 'goes', 'gone', 'got', 'gotten', 'h', 'had', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', 'hed', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'hes', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'home', 'how', 'howbeit', 'however', 'hundred', 'i', 'id', 'ie', 'if', \"i'll\", 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inc', 'indeed', 'index', 'information', 'instead', 'into', 'invention', 'inward', 'is', \"isn't\", 'it', 'itd', \"it'll\", 'its', 'itself', \"i've\", 'j', 'just', 'k', 'keep\\tkeeps', 'kept', 'kg', 'km', 'know', 'known', 'knows', 'l', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'lets', 'like', 'liked', 'likely', 'line', 'little', \"'ll\", 'look', 'looking', 'looks', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'merely', 'mg', 'might', 'million', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'mug', 'must', 'my', 'myself', 'n', 'na', 'name', 'namely', 'nay', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'now', 'nowhere', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'ord', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'page', 'pages', 'part', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'put', 'q', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'run', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sent', 'seven', 'several', 'shall', 'she', 'shed', \"she'll\", 'shes', 'should', \"shouldn't\", 'show', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'since', 'six', 'slightly', 'so', 'some', 'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure\\tt', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \n",
    "             'thered', 'therefore', 'therein', \"there'll\", 'thereof', 'therere', 'theres', 'thereto', 'thereupon', \"there've\", 'these', 'they', 'theyd', \"they'll\", 'theyre', \"they've\", 'think', 'this', 'those', 'thou', 'though', 'though', 'thousand', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'tip', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'v', 'value', 'various', \"'ve\", 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'w', 'want', 'wants', 'was', 'wasnt', 'way', 'we', 'wed', 'welcome', \"we'll\", 'went', 'were', 'werent', \"we've\", 'what', 'whatever', \"what'll\", 'whats', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', \"who'll\", 'whom', 'whomever', 'whos', 'whose', 'why', 'widely', 'willing', 'wish', 'with', 'within', 'without', 'wont', 'words', 'world', 'would', 'wouldnt', 'www', 'x', 'y', 'yes', 'yet', 'you', 'youd', \"you'll\", 'your', 'youre', 'yours', 'yourself', 'yourselves', \"you've\", 'z', 'zero']\n",
    "\n",
    "        \n",
    "\n",
    "stopwords_305 = stopwrd1\n",
    "stopwords_747= list(set(stopwrd1 + stopwords2+ stopwords3 +stopwords4))\n",
    "stopwords_710= stopwords_747[:710]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:54:16.031450Z",
     "start_time": "2019-02-24T15:54:16.024644Z"
    }
   },
   "outputs": [],
   "source": [
    "def  predict(train_features, train_label, test_features): \n",
    "    \"\"\"\n",
    "    input: features for train and test instances and label for train instances\n",
    "    \n",
    "    returns prediction for instance and coefficient matrix and classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    # played around with smoothing 0.1 seemed to be the best\n",
    "    clf = MultinomialNB(alpha=0.1) #using the same classifier as Fox et al.\n",
    "    clf.fit(train_features, train_label)\n",
    "    pred=clf.predict(test_features)\n",
    "    coef = clf.coef_\n",
    "    return pred, coef, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LOO_instances(dataframe):\n",
    "    \"\"\"\n",
    "    \n",
    "    generates instances for LOO\n",
    "    \n",
    "    input: df\n",
    "    \n",
    "    output: list with instances as tuples: [(train_row, test_row)]\n",
    "    \n",
    "    \"\"\"\n",
    "    loo_instances=[]\n",
    "    for instance in range(dataframe.shape[0]):\n",
    "        inst= list(range(dataframe.shape[0]))\n",
    "        inst.remove(instance)\n",
    "        test=dataframe.drop(inst)\n",
    "        train= dataframe.drop([instance])\n",
    "        loo_instances.append((train, test))\n",
    "    return loo_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in given data frame\n",
    "df = pd.read_excel(\"DataFrame.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Play_id</th>\n",
       "      <th>Raw Text</th>\n",
       "      <th>POS_305</th>\n",
       "      <th>POS_710</th>\n",
       "      <th>POS_747</th>\n",
       "      <th>POSstops_305</th>\n",
       "      <th>POSstops_710</th>\n",
       "      <th>POSstops_747</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Label</th>\n",
       "      <th>Author</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As I remember , Adam , it was upon this fashio...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT VERB NOUN SPA...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it VERB upon ...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>as -PRON- remember , adam , -PRON- be upon th...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Proceed , Solinus , to procure my fall\\nAnd by...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>proceed , solinus , to procure -PRON- fall \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>comedy_errors.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Who 's there ?\\nNay , answer me : stand , and ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>who be there ? \\n nay , answer -PRON- : stand...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>Hamlet.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>So shaken as we are , so wan with care ,\\nFind...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>so shake as -PRON- be , so wan with care , \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart1.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Open your ears ; for which of you will stop\\nT...</td>\n",
       "      <td>VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...</td>\n",
       "      <td>VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...</td>\n",
       "      <td>VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will VE...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will st...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will st...</td>\n",
       "      <td>open -PRON- ear ; for which of -PRON- will st...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart2.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Play_id                                           Raw Text  \\\n",
       "0           0        0  As I remember , Adam , it was upon this fashio...   \n",
       "1           1        1  Proceed , Solinus , to procure my fall\\nAnd by...   \n",
       "2           2        2  Who 's there ?\\nNay , answer me : stand , and ...   \n",
       "3           3        3  So shaken as we are , so wan with care ,\\nFind...   \n",
       "4           4        4  Open your ears ; for which of you will stop\\nT...   \n",
       "\n",
       "                                             POS_305  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...   \n",
       "\n",
       "                                             POS_710  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT VERB NOUN SPA...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...   \n",
       "\n",
       "                                             POS_747  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...   \n",
       "\n",
       "                                        POSstops_305  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will VE...   \n",
       "\n",
       "                                        POSstops_710  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it VERB upon ...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will st...   \n",
       "\n",
       "                                        POSstops_747  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will st...   \n",
       "\n",
       "                                              Lemmas  Label         Author  \\\n",
       "0   as -PRON- remember , adam , -PRON- be upon th...      0  E-Shakespeare   \n",
       "1   proceed , solinus , to procure -PRON- fall \\n...      0  E-Shakespeare   \n",
       "2   who be there ? \\n nay , answer -PRON- : stand...      0  E-Shakespeare   \n",
       "3   so shake as -PRON- be , so wan with care , \\n...      0  E-Shakespeare   \n",
       "4   open -PRON- ear ; for which of -PRON- will st...      0  E-Shakespeare   \n",
       "\n",
       "                                  Play  \n",
       "0    asyoulikeit.txt.E-Shakespeare.tok  \n",
       "1  comedy_errors.txt.E-Shakespeare.tok  \n",
       "2         Hamlet.txt.E-Shakespeare.tok  \n",
       "3   henryivPart1.txt.E-Shakespeare.tok  \n",
       "4   henryivPart2.txt.E-Shakespeare.tok  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_imp(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    change function get_features_GM(train_data, test_data) for feature engineering!\n",
    "    input: Dataframe\n",
    "    \n",
    "    output: predictions for test instances, and coefficient matrix, cls\n",
    "    \"\"\"\n",
    "    \n",
    "    instances= get_LOO_instances(dataframe)\n",
    "    predictions=[]\n",
    "    for inst in instances:\n",
    "        train_data= inst[0]\n",
    "        test_data = inst[1]\n",
    "        train_label=train_data[\"Label\"]\n",
    "        train_features, test_features= get_features_GM_imp(train_data, test_data)\n",
    "        pred, coeff, cls= predict(train_features, train_label, test_features)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    return np.array(predictions), coeff, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T17:07:13.874996Z",
     "start_time": "2019-02-21T17:05:22.361790Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T17:07:13.881966Z",
     "start_time": "2019-02-21T17:07:13.876981Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred_imp\"]=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T17:07:13.897924Z",
     "start_time": "2019-02-21T17:07:13.884958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051948051948052\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_imp\"] =df[\"Label\"]==df[ \"Pred_imp\"]\n",
    "acc_imp=df.loc[df.acc_imp== True, 'acc_imp'].count()/df.shape[0]\n",
    "print(acc_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "# if ngrams not restricted: 0.8181818181818182\n",
    "# if ngrams restricted to most frequent 1000: 0.7792207792207793\n",
    "\n",
    "#710 stop words\n",
    "#if ngrams not restricted: 80.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add simple word frequency to the basic features rather than using it by itself as in the bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including word frequency\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "   \n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), max_features = 1000, strip_accents=\"ascii\") #ngrams of POS/stops\n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\") \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_txt).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_txt).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_word_freq, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_word_freq\"]=predictions_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051948051948052\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_word_freq\"] =df[\"Label\"]==df[ \"Pred_word_freq\"]\n",
    "acc_word_freq=df.loc[df.acc_word_freq== True, 'acc_word_freq'].count()/df.shape[0]\n",
    "print(acc_word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#if ngrams not restricted: 0.7662337662337663\n",
    "#if ngrams restricted to most frequent 1000: 0.8181818181818182\n",
    "\n",
    "#710 stop words\n",
    "#if ngrams not restricted: 77.9\n",
    "#if ngrams restricted: 80.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses package RAKE (Rapid Automated Keyword Extraction) to extract keywords and key phrases from the text which can then be counted to form a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get set out of 1000 highest ranked keywords for each play saved in one single string\"\"\"\n",
    "\n",
    "import RAKE\n",
    "rake = RAKE.Rake(stopwords_305) #use stopwords with best results from before\n",
    "\n",
    "all_keywords = []\n",
    "punctuation = \"\"\"!\"',;:.-?)([]<>*#\\n\\t\\r \"\"\"\n",
    "\n",
    "for text in df[\"Raw Text\"]:\n",
    "    keywords = \"\"\n",
    "    text_keywords = rake.run(text.strip(punctuation))\n",
    "    text_keywords = set([keywords[0] for keywords in sorted(text_keywords, key=lambda word: word[1])[:1000]])\n",
    "    for keyword in text_keywords:\n",
    "        keywords += keyword + \" \"\n",
    "    \n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "all_keywords = np.array(all_keywords)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create column and save in df\n",
    "df[\"Keywords\"] = all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including keywords\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    X_key = X_train[\"Keywords\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    X_test_key = X_test[\"Keywords\"]\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features=1000) #ngrams of POS/stops\n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\") #keyword frequency\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_key).toarray(),\n",
    "        \n",
    "        ))\n",
    "\n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_key).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_key, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_keywords\"]=predictions_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_keywords\"] =df[\"Label\"]==df[ \"Pred_keywords\"]\n",
    "acc_keywords=df.loc[df.acc_keywords== True, 'acc_keywords'].count()/df.shape[0]\n",
    "print(acc_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#if ngrams not restricted: 0.7922077922077922\n",
    "#if ngrams restricted to most frequent 1000: 0.8571428571428571\n",
    "\n",
    "#710 stop words\n",
    "#if ngrams restricted: 0.8441558441558441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Frequency of Stemmed Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different nltk stemmers were tested to add frequency of stemmed words as a feature. The Porter Stemmer seemed to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transform given play to a stemmed version and save in df\"\"\"\n",
    "\n",
    "#three different nltk stemmers to be tested\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "punctuation = \"\"\"!\"',;:.-?)([]<>*#\\n\\t\\r \"\"\"\n",
    "all_stemmed_texts = []\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "#change this to test different stemmers\n",
    "stemmer = porter\n",
    "\n",
    "for text in df[\"Raw Text\"]:\n",
    "    stemmed_text = \"\"\n",
    "    for word in text.split():\n",
    "        stemmed_text += stemmer.stem(word.strip(punctuation)) + \" \"\n",
    "    \n",
    "    all_stemmed_texts.append(stemmed_text)\n",
    "\n",
    "all_stemmed_texts = np.array(all_stemmed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save Stem column in df\n",
    "df[\"Stems\"] = all_stemmed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including stemmed words frequency\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    X_stem = X_train[\"Stems\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    X_test_stem = X_test[\"Stems\"]\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features = 1000) #ngrams of POS/stops \n",
    "    \n",
    "    cvec4 = CountVectorizer()\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_stem).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_stem).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_stem, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_stems\"]=predictions_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441558441558441\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_stems\"] =df[\"Label\"]==df[ \"Pred_stems\"]\n",
    "acc_stems=df.loc[df.acc_stems== True, 'acc_stems'].count()/df.shape[0]\n",
    "print(acc_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#restricted ngrams: 0.8311688311688312\n",
    "#unrestricted ngrams: 0.8051948051948052\n",
    "\n",
    "#710 stop words\n",
    "#if ngrams restricted: 0.8441558441558441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Line Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A structural feature representing the frequency of all occurring line lengths.\n",
    "Seems to make the classification a lot worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lengths as list\n",
    "\n",
    "all_linelengths = []\n",
    "max_line_length = max([len(text.split('\\n')) for text in df[\"Raw Text\"]])\n",
    "\n",
    "for text in df[\"Raw Text\"]:\n",
    "    line_lengths_list = []\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        line_lengths_list.append(len(line))\n",
    "    for remain_lines in range(max_line_length - len(line_lengths_list)):\n",
    "        line_lengths_list.append(0)\n",
    "    \n",
    "    all_linelengths.append(line_lengths_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Line Length\"] = all_linelengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including line lengths\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    X_lines = list(X_train[\"Line Length\"])\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    X_test_lines = list(X_test[\"Line Length\"])\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features = 1000) #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        np.asarray(X_lines),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            np.asarray(X_test_lines),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lines, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_lines\"]=predictions_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7012987012987013\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_lines\"] =df[\"Label\"]==df[ \"Pred_lines\"]\n",
    "acc_lines=df.loc[df.acc_lines== True, 'acc_lines'].count()/df.shape[0]\n",
    "print(acc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#restricted ngrams: 0.7142857142857143\n",
    "#unrestricted ngrams: 0.6753246753246753\n",
    "\n",
    "#710 stop words\n",
    "#restricted ngrams: 0.7012987012987013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "tok = PunktSentenceTokenizer()\n",
    "all_sent_lengths = []\n",
    "max_sent_length = max([len(tok.sentences_from_text(text)) for text in df[\"Raw Text\"]])\n",
    "\n",
    "            \n",
    "    \n",
    "for text in df[\"Raw Text\"]:\n",
    "    sent_lengths = []\n",
    "    for sent in tok.sentences_from_text(text):\n",
    "        sent_lengths.append(len(sent))\n",
    "    for remain_sents in range(max_sent_length - len(sent_lengths)):\n",
    "        sent_lengths.append(0)  \n",
    "    \n",
    "    all_sent_lengths.append(sent_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sent Length\"] = all_sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including line lengths\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    X_sent = list(X_train[\"Sent Length\"])\n",
    "\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    X_test_sent = list(X_test[\"Sent Length\"])\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        np.asarray(X_sent),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            np.asarray(X_test_sent),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sent, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_sent\"]=predictions_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4675324675324675\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_sent\"] =df[\"Label\"]==df[ \"Pred_sent\"]\n",
    "acc_sent=df.loc[df.acc_sent== True, 'acc_sent'].count()/df.shape[0]\n",
    "print(acc_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#restricted ngrams: 0.45454545454545453\n",
    "#unrestricted ngrams: 0.45454545454545453\n",
    "\n",
    "#710 stop words\n",
    "#unrestricted ngrams: 0.4675324675324675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Frequency of Lemmatized Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas of words are already to be found in the df (generated with spacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including lemma frequency\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    X_lemmas = X_train[\"Lemmas\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    X_test_lemmas = X_test[\"Lemmas\"]\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_710, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\")\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_lemmas).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_lemmas).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lemmas, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_lemmas\"]=predictions_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_lemmas\"] =df[\"Label\"]==df[ \"Pred_lemmas\"]\n",
    "acc_lemmas=df.loc[df.acc_lemmas== True, 'acc_lemmas'].count()/df.shape[0]\n",
    "print(acc_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stop words\n",
    "#restricted ngrams: 0.8441558441558441\n",
    "\n",
    "#710 stop words\n",
    "#restricted ngrams: 0.8441558441558441\n",
    "#unrestricted ngrams: 0.7792207792207793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Hapax Legomena Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lexical feature calculating the frequency of Hapax Legomena in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(words):\n",
    "    \"\"\"Return the average length of all words in text.\"\"\"\n",
    "    lettersum = 0\n",
    "    for x in words:\n",
    "        lettersum = lettersum + len(x)\n",
    "        \n",
    "    return [lettersum / len(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hapax_legomena_ratio(words):\n",
    "    \"\"\"Return the hapax_legomena ratio for this list of words.\"\"\"\n",
    "    adict = {}\n",
    "    hapaxlego = []\n",
    "    for x in words:\n",
    "        if x not in adict:\n",
    "            adict[x] = 1\n",
    "        else:\n",
    "            adict[x] = adict[x] + 1\n",
    "    for x in adict:\n",
    "        if adict[x] == 1:\n",
    "            hapaxlego.append(x)\n",
    "    hapaxlegoratio = ( len(hapaxlego) / len(words) )\n",
    "    \n",
    "    return [hapaxlegoratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate both hapax legomena and word length for next feature\n",
    "hapax_values = []\n",
    "avg_wordlength_values = []\n",
    "for play in df[\"Raw Text\"]:\n",
    "    processedplay = [word.lower() for word in tokenizer.tokenize(play)]\n",
    "    hapax_values.append(hapax_legomena_ratio(processedplay))\n",
    "    avg_wordlength_values.append(average_word_length(processedplay))\n",
    "\n",
    "df[\"Average Word Length\"] = avg_wordlength_values\n",
    "df[\"Hapax Legomena Ratio\"] = hapax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified get_feature_GM_imp function including hapax legomena ratio\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[ \"POS_305\"]\n",
    "    X_POSstops = X_train[ \"POSstops_305\"]\n",
    "    X_Hapax = list(X_train[\"Hapax Legomena Ratio\"])\n",
    "    \n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[ \"POS_305\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_305\"]\n",
    "    X_test_Hapax = list(X_test[\"Hapax Legomena Ratio\"])\n",
    "    \n",
    "\n",
    "    #using countvectorizors to get freqs \n",
    "    #cvec1 = CountVectorizer( max_features=1000, strip_accents=\"ascii\") \n",
    "    cvec2 = CountVectorizer(stop_words= stopwords_305,  max_df=100, max_features=1000, strip_accents=\"ascii\")\n",
    "     \n",
    "    cvec1 = CountVectorizer(max_features = 1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    #cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    #cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "\n",
    "    # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        #cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        np.asarray(X_Hapax),\n",
    "        ))\n",
    "    \n",
    "\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            #cvec3.transform(X_test_POSstops).toarray(),\n",
    "            np.asarray(X_test_Hapax),\n",
    "        ))  \n",
    "    \n",
    "    \n",
    "\n",
    "    return train_features, test_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_hapax, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_hapax\"]=predictions_hapax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311688311688312\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_hapax\"] =df[\"Label\"]==df[ \"Pred_hapax\"]\n",
    "acc_hapax=df.loc[df.acc_hapax== True, 'acc_hapax'].count()/df.shape[0]\n",
    "print(acc_hapax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stopwords\n",
    "#0.8181818181818182\n",
    "\n",
    "#combination of word frequency, POS and hapax (but also works without the hapax)\n",
    "#0.8311688311688312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Average Word Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lexical feature calculating the average word length in a given corpus. Has been already appended to the df in feature from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified get_feature_GM_imp function including average word length\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_305\"]\n",
    "    X_POSstops = X_train[ \"POSstops_305\"]\n",
    "    X_AvgWordlength = list(X_train[\"Average Word Length\"])\n",
    "    \n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_305\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_305\"]\n",
    "    X_test_AvgWordlength = list(X_test[\"Average Word Length\"])\n",
    "    \n",
    "    \n",
    "    #using countvectorizors to get freqs \n",
    "    #cvec1 = CountVectorizer( max_features=1000, strip_accents=\"ascii\") \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_305,  strip_accents=\"ascii\")\n",
    "     \n",
    "    #cvec1 = CountVectorizer(max_features = 1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "    # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        np.asarray(X_AvgWordlength),\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            np.asarray(X_test_AvgWordlength),\n",
    "        ))  \n",
    "    \n",
    "\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_word_length, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_word_length\"]=predictions_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_word_length\"] =df[\"Label\"]==df[ \"Pred_word_length\"]\n",
    "acc_word_length=df.loc[df.acc_word_length== True, 'acc_word_length'].count()/df.shape[0]\n",
    "print(acc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#305 stopwords\n",
    "#0.8181818181818182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Features that Increased Accuracy Most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting around the basic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including a combination of stemming, lemmatizing, keyword extraction\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_747\"]\n",
    "    X_POSstops = X_train[ \"POSstops_747\"]\n",
    "    X_lemmas = X_train[\"Lemmas\"]\n",
    "    X_keys = X_train[\"Keywords\"]\n",
    "    X_stems = X_train[\"Stems\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_747\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_747\"]\n",
    "    X_test_lemmas = X_test[\"Lemmas\"]\n",
    "    X_test_keys = X_test[\"Keywords\"]\n",
    "    X_test_stems = X_test[\"Stems\"]\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_747, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features = 1000) #ngrams of POS/stops \n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\")\n",
    "    cvec5 = CountVectorizer(strip_accents=\"ascii\")\n",
    "    cvec6 = CountVectorizer()\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_lemmas).toarray(),\n",
    "        cvec5.fit_transform(X_keys).toarray(),\n",
    "        #cvec6.fit_transform(X_stems).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_lemmas).toarray(),\n",
    "            cvec5.transform(X_test_keys).toarray(),\n",
    "            #cvec6.transform(X_test_stems).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_combo5, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_combo5\"]=predictions_combo5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_combo5\"] =df[\"Label\"]==df[ \"Pred_combo5\"]\n",
    "acc_combo5=df.loc[df.acc_combo5== True, 'acc_combo5'].count()/df.shape[0]\n",
    "print(acc_combo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combo 1: basic features with stemming, lemmatizing, keywords, restricted ngrams, 305 stop words\n",
    "#0.7662337662337663\n",
    "\n",
    "#combo 2: combo 1 without stop word freq and POS freq\n",
    "#0.7402597402597403\n",
    "\n",
    "#combo 3: only lemmas and keywords\n",
    "#0.6883116883116883\n",
    "\n",
    "#it's just getting worse\n",
    "\n",
    "#combo 4: 747 stop words, basic features, lemmas\n",
    "#0.8441558441558441\n",
    "\n",
    "#combo 5: 747 stop words, basic features, lemmas, keywords\n",
    "#0.8181818181818182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting without the basic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified get_feature_GM_imp function\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[ \"POS_305\"]\n",
    "    X_POSstops = X_train[ \"POSstops_305\"]\n",
    "    X_keys = X_train[\"Keywords\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[ \"POS_305\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_305\"]\n",
    "    X_test_keys = X_test[\"Keywords\"]\n",
    "    \n",
    "    \n",
    "\n",
    "    #using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer( max_features=1000, strip_accents=\"ascii\") \n",
    "    cvec2 = CountVectorizer(stop_words= stopwords_305,  max_df=100, max_features=1000, strip_accents=\"ascii\")\n",
    "    cvec3 = CountVectorizer(strip_accents = \"ascii\")\n",
    "    \n",
    "    #cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    #cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "\n",
    "    # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POSstops).toarray(),\n",
    "        cvec3.fit_transform(X_keys).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POSstops).toarray(),\n",
    "            cvec3.transform(X_test_keys).toarray(),\n",
    "            \n",
    "        ))  \n",
    "    \n",
    "    \n",
    "\n",
    "    return train_features, test_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tryout, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_tryout\"]=predictions_tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_tryout\"] =df[\"Label\"]==df[ \"Pred_tryout\"]\n",
    "acc_tryout=df.loc[df.acc_tryout== True, 'acc_tryout'].count()/df.shape[0]\n",
    "print(acc_tryout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word freq, POS, lemmas\n",
    "#0.7662337662337663\n",
    "\n",
    "#lemmas, POS\n",
    "#0.7012987012987013\n",
    "\n",
    "#word freq, POS, keywords\n",
    "#0.7792207792207793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to improve the best model (basic features and keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function \n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_747\"]\n",
    "    X_POSstops = X_train[ \"POSstops_747\"]\n",
    "    X_key = X_train[\"Keywords\"]\n",
    "    X_lemmas = X_train[\"Lemmas\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_747\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_747\"]\n",
    "    X_test_key = X_test[\"Keywords\"]\n",
    "    X_test_lemmas = X_test[\"Lemmas\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_747, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer(max_features = 1000) #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features=1000) #ngrams of POS/stops\n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\") #keyword frequency\n",
    "    cvec5 = CountVectorizer(stop_words = stopwords_305, max_features = 500)\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_key).toarray(),\n",
    "        cvec5.fit_transform(X_lemmas).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_key).toarray(),\n",
    "            cvec5.transform(X_test_lemmas).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tryout, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_tryout\"]=predictions_tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441558441558441\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_tryout\"] =df[\"Label\"]==df[ \"Pred_tryout\"]\n",
    "acc_tryout=df.loc[df.acc_tryout== True, 'acc_tryout'].count()/df.shape[0]\n",
    "print(acc_tryout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic, keywords\n",
    "#0.8571428571428571\n",
    "\n",
    "#basic - POS, keywords\n",
    "#0.8311688311688312\n",
    "\n",
    "#basic (restricting stop words), keywords\n",
    "#0.8571428571428571\n",
    "\n",
    "#word freq, POS, ngrams, keywords\n",
    "#0.7792207792207793\n",
    "\n",
    "#basic, keywords, lemmas with removed stopwords\n",
    "#0.7792207792207793\n",
    "\n",
    "#basic (restricting POS, restricting ngrams to 500, keywords)\n",
    "#0.8571428571428571\n",
    "\n",
    "#basic (restricting POS, restricting ngrams to 500 and using 3-grams, keywords)\n",
    "#0.8181818181818182\n",
    "\n",
    "#basic (restricting POS, restricting ngrams to 500 and using bigrams and 3-grams, keywords)\n",
    "#0.8181818181818182\n",
    "\n",
    "#basic with 747 stop words, keywords\n",
    "#0.8571428571428571\n",
    "\n",
    "#basic with 747 stop words, keywords, lemmas with removed stop words and restricted to 500\n",
    "#0.8441558441558441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Best Model to Predict the Author for <i> A Second Maiden's Tragedy <i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fox et al. had <i> A Second Maiden's Tragedy </i> included in their corpus with Middleton as its true author. To ensure the comparability of our models with those of Fox et al., we left it in the corpus when evaluating our model. Now we can have a look at what our overall best model says about the attribution of the play in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#modified get_features function including keywords\n",
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model (created with the LOO function), both for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I want to use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_747\"]\n",
    "    X_POSstops = X_train[ \"POSstops_747\"]\n",
    "    X_key = X_train[\"Keywords\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_747\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_747\"]\n",
    "    X_test_key = X_test[\"Keywords\"]\n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary = stopwords_747, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "    #add max_feature=1000 to get different results\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features=1000) #ngrams of POS/stops\n",
    "    \n",
    "    cvec4 = CountVectorizer(strip_accents=\"ascii\") #keyword frequency\n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        cvec4.fit_transform(X_key).toarray(),\n",
    "        \n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "            cvec4.transform(X_test_key).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_best, coeff, cls =  get_preds_imp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Pred_best\"]=predictions_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "df[\"acc_best\"] =df[\"Label\"]==df[ \"Pred_best\"]\n",
    "acc_best=df.loc[df.acc_best== True, 'acc_best'].count()/df.shape[0]\n",
    "print(acc_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play is most likely written by Middleton\n"
     ]
    }
   ],
   "source": [
    "if df.at[50, 'acc_best'] == True:\n",
    "    print(\"Play is most likely written by Middleton\")\n",
    "else:\n",
    "    print(\"Play is most likely written by author with label {}\".format(df.at[50, 'Pred_best']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributing <i> A Yorkeshire Tragedy </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This play is considered to be a collaboration and likely to include Shakespeare and Middleton amongst its authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in text\n",
    "with open('yorkshire.txt') as text:\n",
    "    yorkshire = text.read()\n",
    "    yorkshire = yorkshire.replace('\\xa0', ' ')\n",
    "    yorkshire = yorkshire.replace('-', '')\n",
    "    yorkshire = yorkshire.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process text in order to compute features\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "y_doc = nlp(yorkshire)\n",
    "\n",
    "y_POS = \"\"\n",
    "y_POSstops = \"\"\n",
    "for word in y_doc:\n",
    "    if word.is_stop:\n",
    "        y_POSstops += str(word) \n",
    "        y_POSstops += str(\" \") \n",
    "    else:\n",
    "        y_POSstops  += str(word.pos_)\n",
    "        y_POSstops += str(\" \")\n",
    "        y_POS += str(word.pos_)\n",
    "        y_POS += \" \"\n",
    "\n",
    "\n",
    "import RAKE\n",
    "rake = RAKE.Rake(stopwords_305) \n",
    "\n",
    "y_keywords = \"\"\n",
    "\n",
    "y_keys = rake.run(yorkshire)\n",
    "y_keys = set([y_key[0] for y_key in sorted(y_keys, key=lambda word: word[1])[:1000]])\n",
    "for y_key in y_keys:\n",
    "    y_keywords += y_key + \" \"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#get features without function since the play is not included in the dataframe\n",
    "\n",
    "\n",
    "X_txt = df[\"Raw Text\"]\n",
    "X_POS = df[\"POS_305\"]\n",
    "X_POSstops = df[ \"POSstops_305\"]\n",
    "X_key = df[\"Keywords\"]\n",
    "    \n",
    "\n",
    "X_test_txt = pd.Series(yorkshire)\n",
    "X_test_POS = pd.Series(y_POS)\n",
    "X_test_POSstops = pd.Series(y_POSstops)\n",
    "X_test_key = pd.Series(y_keywords)\n",
    "\n",
    "\n",
    "cvec1 = CountVectorizer(vocabulary = stopwords_305, strip_accents=\"ascii\")#word freq of stop words\n",
    "cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "#add max_feature=1000 to get different results\n",
    "cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features=1000) #ngrams of POS/stops\n",
    "    \n",
    "cvec4 = CountVectorizer(strip_accents=\"ascii\") #keyword frequency\n",
    "\n",
    "# fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "train_features= np.hstack((\n",
    "    cvec1.fit_transform(X_txt).toarray(),\n",
    "    cvec2.fit_transform(X_POS).toarray(),\n",
    "    cvec3.fit_transform(X_POSstops).toarray(),\n",
    "    cvec4.fit_transform(X_key).toarray(),\n",
    "        \n",
    "    ))\n",
    "\n",
    "\n",
    "#only fit X_test data\n",
    "test_features = np.hstack((\n",
    "        cvec1.transform(X_test_txt).toarray(),\n",
    "        cvec2.transform(X_test_POS).toarray(),\n",
    "        cvec3.transform(X_test_POSstops).toarray(),\n",
    "        cvec4.transform(X_test_key).toarray(),\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "train_label=df[\"Label\"]\n",
    "pred, coeff, cls= predict(train_features, train_label, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play is most likely written by author with label [5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Play is most likely written by author with label {}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model says that <i> A Yorkshire Tragedy </i> has most likely been written by Chapman though he is not even considered in the debate about the authorship of this play. Therefore it doesn't seem likely that our model is correct in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributing <i> The Puritan </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what our model says about a play that is not included in the Fox corpus but has often been attributed to Middleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in text\n",
    "with open('puritan.txt') as text:\n",
    "    puritan = text.read()\n",
    "    puritan = puritan.replace('\\xa0', ' ')\n",
    "    puritan = puritan.replace('-', '')\n",
    "    puritan = puritan.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process text in order to compute features\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "p_doc = nlp(puritan)\n",
    "\n",
    "p_POS = \"\"\n",
    "p_POSstops = \"\"\n",
    "for word in p_doc:\n",
    "    if word.is_stop:\n",
    "        p_POSstops += str(word) \n",
    "        p_POSstops += str(\" \") \n",
    "    else:\n",
    "        p_POSstops  += str(word.pos_)\n",
    "        p_POSstops += str(\" \")\n",
    "        p_POS += str(word.pos_)\n",
    "        p_POS += \" \"\n",
    "\n",
    "\n",
    "import RAKE\n",
    "rake = RAKE.Rake(stopwords_305) \n",
    "\n",
    "p_keywords = \"\"\n",
    "\n",
    "p_keys = rake.run(puritan)\n",
    "p_keys = set([p_key[0] for p_key in sorted(p_keys, key=lambda word: word[1])[:1000]])\n",
    "for p_key in p_keys:\n",
    "    p_keywords += p_key + \" \"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "#get features without function since the play is not included in the dataframe\n",
    "\n",
    "\n",
    "X_txt = df[\"Raw Text\"]\n",
    "X_POS = df[\"POS_305\"]\n",
    "X_POSstops = df[ \"POSstops_305\"]\n",
    "X_key = df[\"Keywords\"]\n",
    "    \n",
    "\n",
    "X_test_txt = pd.Series(puritan)\n",
    "X_test_POS = pd.Series(p_POS)\n",
    "X_test_POSstops = pd.Series(p_POSstops)\n",
    "X_test_key = pd.Series(p_keywords)\n",
    "\n",
    "\n",
    "cvec1 = CountVectorizer(vocabulary = stopwords_305, strip_accents=\"ascii\")#word freq of stop words\n",
    "cvec2 = CountVectorizer() #freq of POS tags\n",
    "    \n",
    "#add max_feature=1000 to get different results\n",
    "cvec3 = CountVectorizer(ngram_range=(2,2), strip_accents=\"ascii\", max_features=1000) #ngrams of POS/stops\n",
    "    \n",
    "cvec4 = CountVectorizer(strip_accents=\"ascii\") #keyword frequency\n",
    "\n",
    "# fitting  and transforming of train data, and stack vectors at the same time to get X for model\n",
    "train_features= np.hstack((\n",
    "    cvec1.fit_transform(X_txt).toarray(),\n",
    "    cvec2.fit_transform(X_POS).toarray(),\n",
    "    cvec3.fit_transform(X_POSstops).toarray(),\n",
    "    cvec4.fit_transform(X_key).toarray(),\n",
    "        \n",
    "    ))\n",
    "\n",
    "\n",
    "#only fit X_test data\n",
    "test_features = np.hstack((\n",
    "        cvec1.transform(X_test_txt).toarray(),\n",
    "        cvec2.transform(X_test_POS).toarray(),\n",
    "        cvec3.transform(X_test_POSstops).toarray(),\n",
    "        cvec4.transform(X_test_key).toarray(),\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "train_label=df[\"Label\"]\n",
    "pred, coeff, cls= predict(train_features, train_label, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play is most likely written by author with label [3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Play is most likely written by author with label {}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It (presumably) correctly states that the play was written by Middleton."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
