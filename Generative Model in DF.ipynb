{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative Model \n",
    "Reimplement using a dataframe to make feature engineering easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:51:45.224984Z",
     "start_time": "2019-02-24T15:51:45.217826Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list with stop words of different length to explore this as a feature\n",
    "Length of lists explored:\n",
    "- 305 used from spacy \n",
    "- 710, which is the same length used by Fox et al. (2014), wihch is combination of different packages\n",
    "- 747, which is the set of all stopword lists combined to get the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T12:58:51.992795Z",
     "start_time": "2019-02-23T12:58:51.011699Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "stopwrd1= []\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    stopwrd1.append(word)\n",
    "\n",
    "\n",
    "stopwords2 = stopwords.words('english')\n",
    "\n",
    "\n",
    "stopwords3 = get_stop_words('english')\n",
    "\n",
    "#from https://www.ranks.nl/stopwords\n",
    "stopwords4 = ['a ', 'able', 'about', 'above', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'again', 'against', 'ah', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apparently', 'approximately', 'are', 'aren', 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asking', 'at', 'auth', 'available', 'away', 'awfully', 'b', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'between', 'beyond', 'biol', 'both', 'brief', 'briefly', 'but', 'by', 'c', 'ca', 'came', 'can', 'cannot', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'co', 'com', 'come', 'comes', 'contain', 'containing', 'contains', 'could', 'couldnt', 'd', 'date', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', 'done', \"don't\", 'down', 'downwards', 'due', 'during', 'e', 'each', 'ed', 'edu', 'effect', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'especially', 'et', 'et-al', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'except', 'f', 'far', 'few', 'ff', 'fifth', 'first', 'five', 'fix', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'found', 'four', 'from', 'further', 'furthermore', 'g', 'gave', 'get', 'gets', 'getting', 'give', 'given', 'gives', 'giving', 'go', 'goes', 'gone', 'got', 'gotten', 'h', 'had', 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', 'hed', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'hes', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'home', 'how', 'howbeit', 'however', 'hundred', 'i', 'id', 'ie', 'if', \"i'll\", 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inc', 'indeed', 'index', 'information', 'instead', 'into', 'invention', 'inward', 'is', \"isn't\", 'it', 'itd', \"it'll\", 'its', 'itself', \"i've\", 'j', 'just', 'k', 'keep\\tkeeps', 'kept', 'kg', 'km', 'know', 'known', 'knows', 'l', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'lets', 'like', 'liked', 'likely', 'line', 'little', \"'ll\", 'look', 'looking', 'looks', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'merely', 'mg', 'might', 'million', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'mug', 'must', 'my', 'myself', 'n', 'na', 'name', 'namely', 'nay', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'now', 'nowhere', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'ord', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'page', 'pages', 'part', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'put', 'q', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'run', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sent', 'seven', 'several', 'shall', 'she', 'shed', \"she'll\", 'shes', 'should', \"shouldn't\", 'show', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'since', 'six', 'slightly', 'so', 'some', 'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure\\tt', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \n",
    "             'thered', 'therefore', 'therein', \"there'll\", 'thereof', 'therere', 'theres', 'thereto', 'thereupon', \"there've\", 'these', 'they', 'theyd', \"they'll\", 'theyre', \"they've\", 'think', 'this', 'those', 'thou', 'though', 'though', 'thousand', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'tip', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'v', 'value', 'various', \"'ve\", 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'w', 'want', 'wants', 'was', 'wasnt', 'way', 'we', 'wed', 'welcome', \"we'll\", 'went', 'were', 'werent', \"we've\", 'what', 'whatever', \"what'll\", 'whats', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', \"who'll\", 'whom', 'whomever', 'whos', 'whose', 'why', 'widely', 'willing', 'wish', 'with', 'within', 'without', 'wont', 'words', 'world', 'would', 'wouldnt', 'www', 'x', 'y', 'yes', 'yet', 'you', 'youd', \"you'll\", 'your', 'youre', 'yours', 'yourself', 'yourselves', \"you've\", 'z', 'zero']\n",
    "\n",
    "        \n",
    "\n",
    "stopwords_305 = stopwrd1\n",
    "stopwords_747= list(set(stopwrd1 + stopwords2+ stopwords3 +stopwords4))\n",
    "stopwords_710= stopwords_747[:710]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T12:58:52.947745Z",
     "start_time": "2019-02-23T12:58:52.943041Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ordered_lists(auhtors, corpus):\n",
    "    \"\"\" \n",
    "    input: corpus as a list with path of files and a list of authors\n",
    "    \n",
    "    output: list of list, with the same order of the author list, with a list containing all plays for each author\n",
    "    \"\"\"\n",
    "    dramas_per_author=[]\n",
    "    for author in auhtors:\n",
    "        authorList=[]\n",
    "        for drama in corpus:\n",
    "            if author in drama:\n",
    "                authorList.append(drama)\n",
    "        dramas_per_author.append(authorList)\n",
    "    \n",
    "    return dramas_per_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T13:14:50.623058Z",
     "start_time": "2019-02-23T12:58:53.298150Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Don't run this! DF is saved and you can open it\"\"\"\n",
    "\n",
    "def create_dataframe_oneText(corpus, authors, stopwrds710, stopwrds747):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: all of the texts top be analysed, all authors, and two lists of stopwords\n",
    "    \n",
    "    runtime: approx 14 min for 76 files \n",
    "    \n",
    "    output: pandas DF with all txt files as one instance: \n",
    "    - Play ID to identify each text, and name of play\n",
    "    - Raw text\n",
    "    - POS tags if word not in respective stop word list\n",
    "    - POS or stop words depentend on stop word list\n",
    "    - lemmas\n",
    "    - auhtor name and label for each author\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en\")\n",
    "    data=[]   \n",
    "    corpus_per_author = get_ordered_lists(authors, corpus)\n",
    "    \n",
    "    play_id = -1\n",
    "    for idx, corpus in enumerate(corpus_per_author):\n",
    "        author=authors[idx]\n",
    "        label=idx\n",
    "\n",
    "        for play in corpus:\n",
    "            play_id += 1\n",
    "\n",
    "            drama=play[3:]\n",
    "            \n",
    "            file= open(play).read()\n",
    "            doc = nlp(file)\n",
    "            \n",
    "            POS305= \" \"\n",
    "            POS710= \" \"\n",
    "            POS747= \" \"\n",
    "            \n",
    "            POS_stwrd305 = \" \"\n",
    "            POSstopwrds710 = \" \"\n",
    "            POSstopwrds747 = \" \"\n",
    "\n",
    "            lemmas= \" \"\n",
    "\n",
    "            for word in doc:\n",
    "                \n",
    "                #getting the lemmas\n",
    "                lemmas += str(word.lemma_)\n",
    "                lemmas += str(\" \")\n",
    "                \n",
    "                #getting either POS or stpwrd with 305 stops\n",
    "                if word.is_stop:\n",
    "                    POS_stwrd305 += str(word) \n",
    "                    POS_stwrd305 += str(\" \") \n",
    "                else:\n",
    "                    POS_stwrd305  += str(word.pos_)\n",
    "                    POS_stwrd305 += str(\" \")\n",
    "                    \n",
    "                #getting either POS or stpwrd with 710 stops                         \n",
    "                if str(word) in stopwrds710:\n",
    "                    POSstopwrds710 += str(word) \n",
    "                    POSstopwrds710 += str(\" \")\n",
    "                else:\n",
    "                    POSstopwrds710  += str(word.pos_)\n",
    "                    POSstopwrds710  += str(\" \")\n",
    "                    \n",
    "                #getting either POS or stpwrd with 747 stops                         \n",
    "                if str(word) in stopwrds747:\n",
    "                    POSstopwrds747 += str(word) \n",
    "                    POSstopwrds747 += str(\" \")\n",
    "                else:\n",
    "                    POSstopwrds747  += str(word.pos_)\n",
    "                    POSstopwrds747  += str(\" \")\n",
    "                    \n",
    "                #getting POS if not stop with 305 stops     \n",
    "                if not word.is_stop:\n",
    "                    POS305 += str(word.pos_)\n",
    "                    POS305 += \" \"\n",
    "                    \n",
    "                #getting POS if not stop with 710 stops     \n",
    "                if str(word) not in stopwrds710:\n",
    "                    POS710 += str(word.pos_)\n",
    "                    POS710 += \" \"\n",
    "\n",
    "                #getting POS if not stop with 710 stops     \n",
    "                if str(word) not in stopwrds747:\n",
    "                    POS747 += str(word.pos_)\n",
    "                    POS747 += \" \"\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "            data.append((play_id, file, POS305, POS710, POS747,  POS_stwrd305, POSstopwrds710, POSstopwrds747,  lemmas, label, author, drama))\n",
    "            \n",
    "    df=pd.DataFrame(data,\n",
    "                    columns=\n",
    "                    [\"Play_id\",\"Raw Text\", \"POS_305\", \"POS_710\", \"POS_747\", \"POSstops_305\", \"POSstops_710\", \"POSstops_747\", \"Lemmas\", \"Label\", \"Author\", \"Play\"] )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "corpus= glob(\"El/*tok*\")\n",
    "y_authors = ['E-Shakespeare', 'L-Shakespeare', 'Marlowe', 'Middleton','Jonson', 'Chapman']\n",
    "df = create_dataframe_oneText(corpus, y_authors, stopwords_710, stopwords_747)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T13:14:50.804729Z",
     "start_time": "2019-02-23T13:14:50.641427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Play_id</th>\n",
       "      <th>Raw Text</th>\n",
       "      <th>POS_305</th>\n",
       "      <th>POS_710</th>\n",
       "      <th>POS_747</th>\n",
       "      <th>POSstops_305</th>\n",
       "      <th>POSstops_710</th>\n",
       "      <th>POSstops_747</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Label</th>\n",
       "      <th>Author</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As I remember , Adam , it was upon this fashio...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT it was upon t...</td>\n",
       "      <td>as -PRON- remember , adam , -PRON- be upon th...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Proceed , Solinus , to procure my fall\\nAnd by...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...</td>\n",
       "      <td>proceed , solinus , to procure -PRON- fall \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>comedy_errors.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Who 's there ?\\nNay , answer me : stand , and ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...</td>\n",
       "      <td>who be there ? \\n nay , answer -PRON- : stand...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>Hamlet.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So shaken as we are , so wan with care ,\\nFind...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>ADV VERB as we are PUNCT so NOUN with NOUN PU...</td>\n",
       "      <td>so shake as -PRON- be , so wan with care , \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart1.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Open your ears ; for which of you will stop\\nT...</td>\n",
       "      <td>VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...</td>\n",
       "      <td>VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...</td>\n",
       "      <td>VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will VE...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will VE...</td>\n",
       "      <td>VERB your NOUN PUNCT for which of you will st...</td>\n",
       "      <td>open -PRON- ear ; for which of -PRON- will st...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart2.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Play_id                                           Raw Text  \\\n",
       "0        0  As I remember , Adam , it was upon this fashio...   \n",
       "1        1  Proceed , Solinus , to procure my fall\\nAnd by...   \n",
       "2        2  Who 's there ?\\nNay , answer me : stand , and ...   \n",
       "3        3  So shaken as we are , so wan with care ,\\nFind...   \n",
       "4        4  Open your ears ; for which of you will stop\\nT...   \n",
       "\n",
       "                                             POS_305  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...   \n",
       "\n",
       "                                             POS_710  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT VERB SPACE DET NOUN VERB ADJ ...   \n",
       "\n",
       "                                             POS_747  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1   PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2   NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3   ADV VERB PUNCT NOUN NOUN PUNCT SPACE VERB NOU...   \n",
       "4   VERB NOUN PUNCT SPACE DET NOUN VERB ADJ PROPN...   \n",
       "\n",
       "                                        POSstops_305  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will VE...   \n",
       "\n",
       "                                        POSstops_710  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will VE...   \n",
       "\n",
       "                                        POSstops_747  \\\n",
       "0   ADP PRON VERB PUNCT PROPN PUNCT it was upon t...   \n",
       "1   PROPN PUNCT PROPN PUNCT to VERB my NOUN SPACE...   \n",
       "2   NOUN VERB there PUNCT SPACE PROPN PUNCT VERB ...   \n",
       "3   ADV VERB as we are PUNCT so NOUN with NOUN PU...   \n",
       "4   VERB your NOUN PUNCT for which of you will st...   \n",
       "\n",
       "                                              Lemmas  Label         Author  \\\n",
       "0   as -PRON- remember , adam , -PRON- be upon th...      0  E-Shakespeare   \n",
       "1   proceed , solinus , to procure -PRON- fall \\n...      0  E-Shakespeare   \n",
       "2   who be there ? \\n nay , answer -PRON- : stand...      0  E-Shakespeare   \n",
       "3   so shake as -PRON- be , so wan with care , \\n...      0  E-Shakespeare   \n",
       "4   open -PRON- ear ; for which of -PRON- will st...      0  E-Shakespeare   \n",
       "\n",
       "                                  Play  \n",
       "0    asyoulikeit.txt.E-Shakespeare.tok  \n",
       "1  comedy_errors.txt.E-Shakespeare.tok  \n",
       "2         Hamlet.txt.E-Shakespeare.tok  \n",
       "3   henryivPart1.txt.E-Shakespeare.tok  \n",
       "4   henryivPart2.txt.E-Shakespeare.tok  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how you display a DF, the default is only 5 rows, to see more just put a number in it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T13:14:55.354606Z",
     "start_time": "2019-02-23T13:14:50.807809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saves data frame to your directory named DataFrame\n",
    "df.to_excel(\"DataFrame.xlsx\")\n",
    "df.shape #df has 77 instnaces (each text one, with 12 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T13:14:56.002802Z",
     "start_time": "2019-02-23T13:14:55.358231Z"
    }
   },
   "outputs": [],
   "source": [
    "#this is how you read an excel file as a DF  \n",
    "df= pd.read_excel(\"DataFrame.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:54:16.031450Z",
     "start_time": "2019-02-24T15:54:16.024644Z"
    }
   },
   "outputs": [],
   "source": [
    "def  predict_NB(train_features, train_label, test_features): \n",
    "    \"\"\"\n",
    "    input: features for train and test instances and label for train instances\n",
    "    \n",
    "    returns prediction for instance and coefficient matrix and classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    # played around with smoothing 0.1 seemed to be the best\n",
    "    clf = MultinomialNB(alpha=0.1) #using the same classifier as Fox et al.\n",
    "    clf.fit(train_features, train_label)\n",
    "    pred=clf.predict(test_features)\n",
    "    coef = clf.coef_\n",
    "    return pred, coef, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:54:16.448513Z",
     "start_time": "2019-02-24T15:54:16.441663Z"
    }
   },
   "outputs": [],
   "source": [
    "def  predict_SVM(train_features, train_label, test_features): \n",
    "    \"\"\"\n",
    "    input: features for train and test instances and label for train instances\n",
    "    \n",
    "    returns prediction for instance and coefficient matrix and classifier\n",
    "    \n",
    "    \"\"\"\n",
    "    # played around with smoothing 0.1 seemed to be the best\n",
    "    clf = LinearSVC() #using the same classifier as Fox et al.\n",
    "    clf.fit(train_features, train_label)\n",
    "    pred=clf.predict(test_features)\n",
    "    coef = clf.coef_\n",
    "    return pred, coef, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T15:58:05.275440Z",
     "start_time": "2019-02-24T15:58:05.158755Z"
    }
   },
   "outputs": [],
   "source": [
    "#I could only find out off the box functions for cross validations bigger than 1, thus I wrote this\n",
    "def get_LOO_instances(dataframe):\n",
    "    \"\"\"\n",
    "    \n",
    "    generates instances for LOO\n",
    "    \n",
    "    input: df\n",
    "    \n",
    "    output: list with instances as tuples: [(train_row, test_row)]\n",
    "    \n",
    "    \"\"\"\n",
    "    loo_instances=[]\n",
    "    for instance in range(dataframe.shape[0]):\n",
    "        inst= list(range(dataframe.shape[0]))\n",
    "        inst.remove(instance)\n",
    "        test=dataframe.drop(inst)\n",
    "        train= dataframe.drop([instance])\n",
    "        loo_instances.append((train, test))\n",
    "    return loo_instances\n",
    "\n",
    "instances = get_LOO_instances(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implenting features used by Fox et al. (2014) as a basis with 710 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:20:34.070341Z",
     "start_time": "2019-02-24T16:20:34.059513Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_GM_710(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, bot for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_710\"]\n",
    "    X_POSstops = X_train[ \"POSstops_710\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_710\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_710\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary= stopwords_710, max_features=1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer(max_features=1000, strip_accents=\"ascii\") #freq of POS taggs\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2),max_features=1000, strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    feature_list= list(cvec1.get_feature_names()) + list(cvec2.get_feature_names()) + list(cvec3.get_feature_names())\n",
    "\n",
    "\n",
    "    \n",
    "    return train_features, test_features, feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:31:51.257822Z",
     "start_time": "2019-02-24T16:31:51.248847Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_710(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    change function get_features_GM(train_data, test_data) for feature engineering!\n",
    "    input: Dataframe\n",
    "    \n",
    "    output: predictions for test instances\n",
    "    \"\"\"\n",
    "    \n",
    "    instances= get_LOO_instances(dataframe)\n",
    "    predictions_NB=[]\n",
    "    predictions_SVM=[]\n",
    "    for inst in instances:\n",
    "        train_data= inst[0]\n",
    "        test_data = inst[1]\n",
    "        train_label=train_data[\"Label\"]\n",
    "        train_features, test_features= get_features_GM_710(train_data, test_data)\n",
    "        pred_NB, coeff_NB, cls_NB = predict_NB(train_features, train_label, test_features)\n",
    "        pred_SVM, coeff_SVM, cls_SVM = predict_SVM(train_features, train_label, test_features)\n",
    "        predictions_NB.append(pred_NB)\n",
    "        predictions_SVM.append(pred_SVM)\n",
    "        \n",
    "        \n",
    "    return np.array(predictions_NB), coeff_NB, cls_NB, np.array(predictions_SVM), coeff_SVM, cls_SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:34:48.069057Z",
     "start_time": "2019-02-24T16:31:51.746071Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_NB_710, coeff_NB_710, cls_NB_710, predictions_SVM_710, coeff_SVM_710, cls_SVM_710  =  get_preds_710(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:34:48.081024Z",
     "start_time": "2019-02-24T16:34:48.074042Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred_NB_710\"]=predictions_NB_710 #I add my predictions to the data frame as a new column\n",
    "df[\"Pred_SVM_710\"]=predictions_SVM_710 #I add my predictions to the data frame as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:34:48.105956Z",
     "start_time": "2019-02-24T16:34:48.086011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_NB_710\"] =df[\"Label\"]==df[ \"Pred_NB_710\"]\n",
    "acc_NB_710 = df.loc[df.acc_NB_710 == True, 'acc_NB_710'].count()/df.shape[0]\n",
    "print(acc_NB_710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:34:48.129893Z",
     "start_time": "2019-02-24T16:34:48.111941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051948051948052\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_SVM_710\"] =df[\"Label\"]==df[ \"Pred_SVM_710\"]\n",
    "acc_SVM_710 = df.loc[df.acc_SVM_710 == True, 'acc_SVM_710'].count()/df.shape[0]\n",
    "print(acc_SVM_710)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take different sets of Stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use list of stop words of length 305 (min of stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:35:06.871113Z",
     "start_time": "2019-02-24T16:35:06.860142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_GM_305(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, bot for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_305\"]\n",
    "    X_POSstops = X_train[ \"POSstops_305\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_305\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_305\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary= stopwords_305, max_features=1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer(max_features=1000, strip_accents=\"ascii\") #freq of POS taggs\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2),max_features=1000, strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:35:07.380452Z",
     "start_time": "2019-02-24T16:35:07.373472Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_305(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    change function get_features_GM(train_data, test_data) for feature engineering!\n",
    "    input: Dataframe\n",
    "    \n",
    "    output: predictions for test instances\n",
    "    \"\"\"\n",
    "    \n",
    "    instances= get_LOO_instances(dataframe)\n",
    "    predictions_NB=[]\n",
    "    predictions_SVM=[]\n",
    "    for inst in instances:\n",
    "        train_data= inst[0]\n",
    "        test_data = inst[1]\n",
    "        train_label=train_data[\"Label\"]\n",
    "        train_features, test_features= get_features_GM_305(train_data, test_data)\n",
    "        pred_NB, coeff_NB, cls_NB = predict_NB(train_features, train_label, test_features)\n",
    "        pred_SVM, coeff_SVM, cls_SVM = predict_SVM(train_features, train_label, test_features)\n",
    "        predictions_NB.append(pred_NB)\n",
    "        predictions_SVM.append(pred_SVM)\n",
    "        \n",
    "        \n",
    "    return np.array(predictions_NB), coeff_NB, cls_NB, np.array(predictions_SVM), coeff_SVM, cls_SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:38:21.019609Z",
     "start_time": "2019-02-24T16:35:08.134010Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_NB_305, coeff_NB_305, cls_NB_305, predictions_SVM_305, coeff_SVM_305, cls_SVM_305  =  get_preds_305(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:39:33.920613Z",
     "start_time": "2019-02-24T16:39:33.913632Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred_NB_305\"]=predictions_NB_710 #I add my predictions to the data frame as a new column\n",
    "df[\"Pred_SVM_305\"]=predictions_SVM_710 #I add my predictions to the data frame as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:39:35.158422Z",
     "start_time": "2019-02-24T16:39:35.146457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_NB_305\"] =df[\"Label\"]==df[ \"Pred_NB_305\"]\n",
    "acc_NB_305 = df.loc[df.acc_NB_305 == True, 'acc_NB_305'].count()/df.shape[0]\n",
    "print(acc_NB_305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:39:36.111026Z",
     "start_time": "2019-02-24T16:39:36.098061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051948051948052\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_SVM_305\"] =df[\"Label\"]==df[ \"Pred_SVM_305\"]\n",
    "acc_SVM_305 = df.loc[df.acc_SVM_305 == True, 'acc_SVM_305'].count()/df.shape[0]\n",
    "print(acc_SVM_305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use list of stop words of length 747 (max of stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:39:50.893984Z",
     "start_time": "2019-02-24T16:39:50.883014Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_GM_747(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, bot for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_747\"]\n",
    "    X_POSstops = X_train[ \"POSstops_747\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_747\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_747\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary= stopwords_747, max_features=1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer(max_features=1000, strip_accents=\"ascii\") #freq of POS taggs\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2),max_features=1000, strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:39:57.526060Z",
     "start_time": "2019-02-24T16:39:57.516087Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_747(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    change function get_features_GM(train_data, test_data) for feature engineering!\n",
    "    input: Dataframe\n",
    "    \n",
    "    output: predictions for test instances\n",
    "    \"\"\"\n",
    "    \n",
    "    instances= get_LOO_instances(dataframe)\n",
    "    predictions_NB=[]\n",
    "    predictions_SVM=[]\n",
    "    for inst in instances:\n",
    "        train_data= inst[0]\n",
    "        test_data = inst[1]\n",
    "        train_label=train_data[\"Label\"]\n",
    "        train_features, test_features= get_features_GM_747(train_data, test_data)\n",
    "        pred_NB, coeff_NB, cls_NB = predict_NB(train_features, train_label, test_features)\n",
    "        pred_SVM, coeff_SVM, cls_SVM = predict_SVM(train_features, train_label, test_features)\n",
    "        predictions_NB.append(pred_NB)\n",
    "        predictions_SVM.append(pred_SVM)\n",
    "        \n",
    "        \n",
    "    return np.array(predictions_NB), coeff_NB, cls_NB, np.array(predictions_SVM), coeff_SVM, cls_SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:43:18.440288Z",
     "start_time": "2019-02-24T16:40:04.118308Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_NB_747, coeff_NB_747, cls_NB_747, predictions_SVM_747, coeff_SVM_747, cls_SVM_747  =  get_preds_747(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:43:51.811354Z",
     "start_time": "2019-02-24T16:43:51.805399Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred_NB_747\"]=predictions_NB_747 #I add my predictions to the data frame as a new column\n",
    "df[\"Pred_SVM_747\"]=predictions_SVM_747 #I add my predictions to the data frame as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:43:52.217394Z",
     "start_time": "2019-02-24T16:43:52.206456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_NB_747\"] =df[\"Label\"]==df[ \"Pred_NB_747\"]\n",
    "acc_NB_747 = df.loc[df.acc_NB_747 == True, 'acc_NB_747'].count()/df.shape[0]\n",
    "print(acc_NB_747)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T16:43:53.049091Z",
     "start_time": "2019-02-24T16:43:53.039089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_SVM_747\"] =df[\"Label\"]==df[ \"Pred_SVM_747\"]\n",
    "acc_SVM_747 = df.loc[df.acc_SVM_747 == True, 'acc_SVM_747'].count()/df.shape[0]\n",
    "print(acc_SVM_747)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:51:13.111486Z",
     "start_time": "2019-02-24T19:51:13.065609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Pred_NB_305</th>\n",
       "      <th>Pred_NB_710</th>\n",
       "      <th>Pred_NB_747</th>\n",
       "      <th>Pred_SVM_305</th>\n",
       "      <th>Pred_SVM_710</th>\n",
       "      <th>Pred_SVM_747</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.779221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label  Pred_NB_305  Pred_NB_710  Pred_NB_747  Pred_SVM_305  Pred_SVM_710  \\\n",
       "0     0.0     1.000000     1.000000     1.000000      0.000000      0.000000   \n",
       "1     0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "2     0.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "3     0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "4     0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "5     0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "6     0.0     2.000000     2.000000     2.000000      2.000000      2.000000   \n",
       "7     0.0     2.000000     2.000000     2.000000      0.000000      0.000000   \n",
       "8     0.0     2.000000     2.000000     2.000000      2.000000      2.000000   \n",
       "9     0.0     0.000000     0.000000     0.000000      1.000000      1.000000   \n",
       "10    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "11    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "12    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "13    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "14    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "15    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "16    0.0     2.000000     2.000000     2.000000      0.000000      0.000000   \n",
       "17    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "18    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "19    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "20    0.0     2.000000     2.000000     2.000000      0.000000      0.000000   \n",
       "21    0.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "22    0.0     0.000000     0.000000     0.000000      0.000000      0.000000   \n",
       "23    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "24    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "25    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "26    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "27    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "28    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "29    1.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "..    ...          ...          ...          ...           ...           ...   \n",
       "48    3.0     3.000000     3.000000     3.000000      3.000000      3.000000   \n",
       "49    3.0     3.000000     3.000000     3.000000      3.000000      3.000000   \n",
       "50    3.0     3.000000     3.000000     3.000000      3.000000      3.000000   \n",
       "51    3.0     3.000000     3.000000     3.000000      3.000000      3.000000   \n",
       "52    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "53    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "54    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "55    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "56    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "57    4.0     0.000000     0.000000     0.000000      1.000000      1.000000   \n",
       "58    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "59    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "60    4.0     1.000000     1.000000     1.000000      1.000000      1.000000   \n",
       "61    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "62    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "63    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "64    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "65    4.0     4.000000     4.000000     4.000000      4.000000      4.000000   \n",
       "66    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "67    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "68    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "69    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "70    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "71    5.0     0.000000     0.000000     0.000000      5.000000      5.000000   \n",
       "72    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "73    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "74    5.0     0.000000     0.000000     0.000000      5.000000      5.000000   \n",
       "75    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "76    5.0     5.000000     5.000000     5.000000      5.000000      5.000000   \n",
       "77  100.0     0.779221     0.779221     0.779221      0.805195      0.805195   \n",
       "\n",
       "    Pred_SVM_747  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       1.000000  \n",
       "3       0.000000  \n",
       "4       0.000000  \n",
       "5       1.000000  \n",
       "6       2.000000  \n",
       "7       0.000000  \n",
       "8       2.000000  \n",
       "9       1.000000  \n",
       "10      0.000000  \n",
       "11      0.000000  \n",
       "12      0.000000  \n",
       "13      0.000000  \n",
       "14      0.000000  \n",
       "15      0.000000  \n",
       "16      0.000000  \n",
       "17      0.000000  \n",
       "18      0.000000  \n",
       "19      0.000000  \n",
       "20      0.000000  \n",
       "21      1.000000  \n",
       "22      1.000000  \n",
       "23      1.000000  \n",
       "24      1.000000  \n",
       "25      1.000000  \n",
       "26      1.000000  \n",
       "27      1.000000  \n",
       "28      1.000000  \n",
       "29      1.000000  \n",
       "..           ...  \n",
       "48      3.000000  \n",
       "49      3.000000  \n",
       "50      3.000000  \n",
       "51      3.000000  \n",
       "52      4.000000  \n",
       "53      4.000000  \n",
       "54      4.000000  \n",
       "55      4.000000  \n",
       "56      4.000000  \n",
       "57      1.000000  \n",
       "58      4.000000  \n",
       "59      4.000000  \n",
       "60      1.000000  \n",
       "61      4.000000  \n",
       "62      4.000000  \n",
       "63      4.000000  \n",
       "64      4.000000  \n",
       "65      4.000000  \n",
       "66      5.000000  \n",
       "67      5.000000  \n",
       "68      5.000000  \n",
       "69      5.000000  \n",
       "70      5.000000  \n",
       "71      5.000000  \n",
       "72      5.000000  \n",
       "73      5.000000  \n",
       "74      5.000000  \n",
       "75      5.000000  \n",
       "76      5.000000  \n",
       "77      0.779221  \n",
       "\n",
       "[78 rows x 7 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stpw_results = pd.DataFrame()\n",
    "df_stpw_results= pd.DataFrame([ df[\"Label\"], df[\"Pred_NB_305\"], df[\"Pred_NB_710\"], df[\"Pred_NB_747\"], df[\"Pred_SVM_305\"], df[\"Pred_SVM_710\"], df[\"Pred_SVM_747\"] ]).transpose().append({\"Label\": 100, \"Pred_NB_305\": acc_NB_305, \"Pred_NB_710\":acc_NB_710, \"Pred_NB_747\": acc_NB_747, \"Pred_SVM_305\": acc_SVM_305, \"Pred_SVM_710\":acc_SVM_710, \"Pred_SVM_747\": acc_SVM_747},  ignore_index=True)\n",
    "df_stpw_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T17:18:08.471446Z",
     "start_time": "2019-02-24T17:18:08.465489Z"
    }
   },
   "source": [
    "__Results__  \n",
    "\n",
    "\n",
    "| n_stop_words \t| NB \t| SVM \t|  \n",
    "|--------------\t|--------\t|--------\t|\n",
    "| *305*  \t| 0.7792 \t| __0.8052__ \t|  \n",
    "| *710* \t| 0.7792 \t| __0.8052__ \t|\n",
    "| *747*  \t| 0.7792 \t| 0.7792 \t| \n",
    "\n",
    "\n",
    "There might be the maximum somewhere between the 305 and 747 stop words. Thus will explore this in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T18:50:21.062287Z",
     "start_time": "2019-02-24T18:50:21.056245Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_507=stopwords_747[:int(305+(710-305)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:15:18.481455Z",
     "start_time": "2019-02-24T19:01:29.073705Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe_improve(corpus, authors, new_stops):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: all of the texts top be analysed, all authors, and a list of stop words\n",
    "    \n",
    "    \n",
    "    output: pandas DF with all txt files as one instance: \n",
    "    - Play ID to identify each text, and name of play\n",
    "    - Raw text\n",
    "    - POS tags if word not in respective stop word list\n",
    "    - POS or stop words depentend on stop word list\n",
    "    - auhtor name and label for each author\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en\")\n",
    "    data=[]   \n",
    "    corpus_per_author = get_ordered_lists(authors, corpus)\n",
    "    \n",
    "    play_id = -1\n",
    "    for idx, corpus in enumerate(corpus_per_author):\n",
    "        author=authors[idx]\n",
    "        label=idx\n",
    "\n",
    "        for play in corpus:\n",
    "            play_id += 1\n",
    "\n",
    "            drama=play[3:]\n",
    "            \n",
    "            file= open(play).read()\n",
    "            doc = nlp(file)\n",
    "            \n",
    "            POS_new= \" \"\n",
    "            POSstopwrds_new = \" \"\n",
    "\n",
    "            for word in doc:\n",
    "                \n",
    "                if str(word) in new_stops:\n",
    "                    POSstopwrds_new += str(word) \n",
    "                    POSstopwrds_new += str(\" \")\n",
    "                else:\n",
    "                    POSstopwrds_new  += str(word.pos_)\n",
    "                    POSstopwrds_new  += str(\" \")\n",
    "                    \n",
    "                if str(word) not in new_stops:\n",
    "                    POS_new += str(word.pos_)\n",
    "                    POS_new += \" \"\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "            data.append((play_id, file, POS_new, POS_new, label, author, drama))\n",
    "            \n",
    "    df=pd.DataFrame(data,\n",
    "                    columns=\n",
    "                    [\"Play_id\",\"Raw Text\", \"POS_new\", \"POSstops_new\", \"Label\", \"Author\", \"Play\"] )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "corpus= glob(\"El/*tok*\")\n",
    "y_authors = ['E-Shakespeare', 'L-Shakespeare', 'Marlowe', 'Middleton','Jonson', 'Chapman']\n",
    "df_imp = create_dataframe_improve(corpus, y_authors, stopwords_507)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:15:18.654187Z",
     "start_time": "2019-02-24T19:15:18.488176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Play_id</th>\n",
       "      <th>Raw Text</th>\n",
       "      <th>POS_new</th>\n",
       "      <th>POSstops_new</th>\n",
       "      <th>Label</th>\n",
       "      <th>Author</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As I remember , Adam , it was upon this fashio...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>asyoulikeit.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Proceed , Solinus , to procure my fall\\nAnd by...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>comedy_errors.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Who 's there ?\\nNay , answer me : stand , and ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>Hamlet.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So shaken as we are , so wan with care ,\\nFind...</td>\n",
       "      <td>ADV VERB VERB PUNCT NOUN NOUN PUNCT SPACE VER...</td>\n",
       "      <td>ADV VERB VERB PUNCT NOUN NOUN PUNCT SPACE VER...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart1.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Open your ears ; for which of you will stop\\nT...</td>\n",
       "      <td>VERB NOUN PUNCT ADP ADJ PRON VERB SPACE DET N...</td>\n",
       "      <td>VERB NOUN PUNCT ADP ADJ PRON VERB SPACE DET N...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryivPart2.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>O for a Muse of fire , that would ascend\\nThe ...</td>\n",
       "      <td>INTJ ADP PROPN NOUN PUNCT VERB SPACE DET ADJ ...</td>\n",
       "      <td>INTJ ADP PROPN NOUN PUNCT VERB SPACE DET ADJ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryv.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Hung be the heavens with black , yield day to ...</td>\n",
       "      <td>PROPN NOUN ADJ PUNCT NOUN NOUN NOUN PUNCT SPA...</td>\n",
       "      <td>PROPN NOUN ADJ PUNCT NOUN NOUN NOUN PUNCT SPA...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryviPart1.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>As by your high imperial majesty\\nI had in cha...</td>\n",
       "      <td>ADP ADP ADJ ADJ NOUN SPACE PRON NOUN NOUN ADP...</td>\n",
       "      <td>ADP ADP ADJ ADJ NOUN SPACE PRON NOUN NOUN ADP...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryviPart2.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I wonder how the king escaped our hands .\\nWhi...</td>\n",
       "      <td>PRON VERB ADV NOUN VERB ADJ NOUN PUNCT SPACE ...</td>\n",
       "      <td>PRON VERB ADV NOUN VERB ADJ NOUN PUNCT SPACE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>henryviPart3.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Hence ! home , you idle creatures get you home...</td>\n",
       "      <td>ADV PUNCT PUNCT PRON ADJ NOUN PRON PUNCT SPAC...</td>\n",
       "      <td>ADV PUNCT PUNCT PRON ADJ NOUN PRON PUNCT SPAC...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>julius_caesar.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Now , say , Chatillon , what would France with...</td>\n",
       "      <td>ADV PUNCT PUNCT PROPN PUNCT PROPN PRON PUNCT ...</td>\n",
       "      <td>ADV PUNCT PUNCT PROPN PUNCT PROPN PRON PUNCT ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>King John.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Let fame , that all hunt after in their lives ...</td>\n",
       "      <td>VERB NOUN PUNCT NOUN ADP ADJ NOUN PUNCT SPACE...</td>\n",
       "      <td>VERB NOUN PUNCT NOUN ADP ADJ NOUN PUNCT SPACE...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>Love_Labours_lost.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>In sooth , I know not why I am so sad :\\nIt we...</td>\n",
       "      <td>ADP NOUN PUNCT PRON VERB ADV PRON VERB ADJ PU...</td>\n",
       "      <td>ADP NOUN PUNCT PRON VERB ADV PRON VERB ADJ PU...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>merchant-venice.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Sir Hugh , persuade me not ; I will make a Sta...</td>\n",
       "      <td>PROPN PROPN PUNCT VERB ADV PUNCT PRON ADJ SPA...</td>\n",
       "      <td>PROPN PROPN PUNCT VERB ADV PUNCT PRON ADJ SPA...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>merry_wives.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Now , fair Hippolyta , our nuptial hour\\nDraws...</td>\n",
       "      <td>ADV PUNCT ADJ PROPN PUNCT ADJ ADJ NOUN SPACE ...</td>\n",
       "      <td>ADV PUNCT ADJ PROPN PUNCT ADJ ADJ NOUN SPACE ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>midsummer.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>I learn in this letter that Don Peter of Arrag...</td>\n",
       "      <td>PRON VERB NOUN PROPN PROPN PROPN SPACE NOUN P...</td>\n",
       "      <td>PRON VERB NOUN PROPN PROPN PROPN SPACE NOUN P...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>much_ado.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Old John of Gaunt , time-honour 'd Lancaster ,...</td>\n",
       "      <td>PROPN PROPN PROPN PUNCT NOUN PUNCT NOUN PROPN...</td>\n",
       "      <td>PROPN PROPN PROPN PUNCT NOUN PUNCT NOUN PROPN...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>richardii.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Now is the winter of our discontent\\nMade glor...</td>\n",
       "      <td>ADV NOUN ADJ NOUN SPACE VERB ADJ NOUN ADP NOU...</td>\n",
       "      <td>ADV NOUN ADJ NOUN SPACE VERB ADJ NOUN ADP NOU...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>richardiii.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Two households , both alike in dignity ,\\nIn f...</td>\n",
       "      <td>NUM NOUN PUNCT ADV NOUN PUNCT SPACE ADP ADJ P...</td>\n",
       "      <td>NUM NOUN PUNCT ADV NOUN PUNCT SPACE ADP ADJ P...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>romeo_juliet.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>I 'll pheeze you , in faith .\\nA pair of stock...</td>\n",
       "      <td>PRON VERB PRON PUNCT NOUN PUNCT SPACE DET NOU...</td>\n",
       "      <td>PRON VERB PRON PUNCT NOUN PUNCT SPACE DET NOU...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>taming_shrew.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Noble patricians , patrons of my right ,\\nDefe...</td>\n",
       "      <td>ADJ NOUN PUNCT NOUN PUNCT SPACE VERB NOUN NOU...</td>\n",
       "      <td>ADJ NOUN PUNCT NOUN PUNCT SPACE VERB NOUN NOU...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>Titus_Andronicus.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>If music be the food of love , play on ;\\nGive...</td>\n",
       "      <td>ADP NOUN NOUN NOUN PUNCT VERB PART PUNCT SPAC...</td>\n",
       "      <td>ADP NOUN NOUN NOUN PUNCT VERB PART PUNCT SPAC...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>twelfth_night.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Cease to persuade , my loving Proteus :\\nHome-...</td>\n",
       "      <td>VERB VERB PUNCT VERB PROPN PUNCT SPACE NOUN P...</td>\n",
       "      <td>VERB VERB PUNCT VERB PROPN PUNCT SPACE NOUN P...</td>\n",
       "      <td>0</td>\n",
       "      <td>E-Shakespeare</td>\n",
       "      <td>two_gentlemen.txt.E-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>In delivering my son from me , I bury a second...</td>\n",
       "      <td>ADP VERB NOUN PUNCT PRON VERB ADJ NOUN PUNCT ...</td>\n",
       "      <td>ADP VERB NOUN PUNCT PRON VERB ADJ NOUN PUNCT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>allswell.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Nay , but this dotage of our general 's\\nO'erf...</td>\n",
       "      <td>INTJ PUNCT NOUN ADJ NOUN PART SPACE VERB NOUN...</td>\n",
       "      <td>INTJ PUNCT NOUN ADJ NOUN PART SPACE VERB NOUN...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>Cleopatra.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Before we proceed any further , hear me speak ...</td>\n",
       "      <td>ADP VERB DET PUNCT VERB VERB PUNCT SPACE PROP...</td>\n",
       "      <td>ADP VERB DET PUNCT VERB VERB PUNCT SPACE PROP...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>coriolanus.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>You do not meet a man but frowns : our bloods\\...</td>\n",
       "      <td>PRON VERB ADV VERB NOUN VERB PUNCT ADJ NOUN S...</td>\n",
       "      <td>PRON VERB ADV VERB NOUN VERB PUNCT ADJ NOUN S...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>Cymbeline.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>I come no more to make you laugh : things now ...</td>\n",
       "      <td>PRON ADJ PRON VERB PUNCT NOUN PUNCT SPACE ADJ...</td>\n",
       "      <td>PRON ADJ PRON VERB PUNCT NOUN PUNCT SPACE ADJ...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>henryviii.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>I thought the king had more affected the Duke ...</td>\n",
       "      <td>PRON VERB NOUN ADV PROPN SPACE PROPN PROPN PU...</td>\n",
       "      <td>PRON VERB NOUN ADV PROPN SPACE PROPN PROPN PU...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>Lear.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>When shall we three meet again\\nIn thunder , l...</td>\n",
       "      <td>ADV VERB VERB SPACE ADP NOUN PUNCT NOUN PUNCT...</td>\n",
       "      <td>ADV VERB VERB SPACE ADP NOUN PUNCT NOUN PUNCT...</td>\n",
       "      <td>1</td>\n",
       "      <td>L-Shakespeare</td>\n",
       "      <td>Macbeth.txt.L-Shakespeare.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>What Raynulph monk of Chester can\\nRaise from ...</td>\n",
       "      <td>NOUN PROPN NOUN PROPN SPACE PROPN SPACE ADJ V...</td>\n",
       "      <td>NOUN PROPN NOUN PROPN SPACE PROPN SPACE ADJ V...</td>\n",
       "      <td>3</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>hengist.txt.Middleton.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>I am at my wit 's end , Savorwit .\\nAnd I am e...</td>\n",
       "      <td>PRON VERB NOUN PART PUNCT PROPN PUNCT SPACE C...</td>\n",
       "      <td>PRON VERB NOUN PART PUNCT PROPN PUNCT SPACE C...</td>\n",
       "      <td>3</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>no wit.txt.Middleton.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>My lords ,\\nKnow that we , far from any natura...</td>\n",
       "      <td>ADJ NOUN PUNCT SPACE VERB PUNCT ADV DET ADJ N...</td>\n",
       "      <td>ADJ NOUN PUNCT SPACE VERB PUNCT ADV DET ADJ N...</td>\n",
       "      <td>3</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>Phoenix.txt.Middleton.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Thus high , my lords , your powers and constan...</td>\n",
       "      <td>ADV ADJ PUNCT NOUN PUNCT NOUN ADJ NOUN SPACE ...</td>\n",
       "      <td>ADV ADJ PUNCT NOUN PUNCT NOUN ADJ NOUN SPACE ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>the second maiden.txt.Middleton.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>My three years spent in war has now undone\\nMy...</td>\n",
       "      <td>ADJ NOUN VERB NOUN VERB SPACE ADJ NOUN ADV PU...</td>\n",
       "      <td>ADJ NOUN VERB NOUN VERB SPACE ADJ NOUN ADV PU...</td>\n",
       "      <td>3</td>\n",
       "      <td>Middleton</td>\n",
       "      <td>the witch.txt.Middleton.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>NOW o ' my Faith , Old Bishop Valen-\\ntine ,\\n...</td>\n",
       "      <td>ADV PUNCT NOUN PUNCT PROPN PROPN PROPN SPACE ...</td>\n",
       "      <td>ADV PUNCT NOUN PUNCT PROPN PROPN PROPN SPACE ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>A Tale of a Tub.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>such luck to spin out these fine things still ...</td>\n",
       "      <td>ADJ NOUN VERB PART DET ADJ NOUN ADV PUNCT SPA...</td>\n",
       "      <td>ADJ NOUN VERB PART DET ADJ NOUN ADV PUNCT SPA...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Bartholmew Fair.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>Sylla 's Ghost .\\nDOst thou not feel me , Rome...</td>\n",
       "      <td>PROPN PART PROPN PUNCT SPACE PROPN ADV VERB P...</td>\n",
       "      <td>PROPN PART PROPN PUNCT SPACE PROPN ADV VERB P...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Catiline His Conspiracy.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>' Tis I , blind Archer .\\nWho ?\\nMercury ?\\nSt...</td>\n",
       "      <td>PUNCT PROPN PRON PUNCT ADJ PROPN PUNCT SPACE ...</td>\n",
       "      <td>PUNCT PROPN PRON PUNCT ADJ PROPN PUNCT SPACE ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Cynthias Revels.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>Brain-worm ,\\nCall up your young master : bid ...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT SPACE VERB ADJ NOUN PUN...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT SPACE VERB ADJ NOUN PUN...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Every Man in his Humour.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>I Would , I could make 'hem a shew my self . I...</td>\n",
       "      <td>PRON VERB PUNCT PRON VERB PUNCT VERB NOUN PUN...</td>\n",
       "      <td>PRON VERB PUNCT PRON VERB PUNCT VERB NOUN PUN...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Love_Restored.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>95\\nP O E T A S T E R:\\nO R,\\nHis Arraignment ...</td>\n",
       "      <td>NUM SPACE NOUN NOUN NOUN NOUN NOUN NOUN NOUN ...</td>\n",
       "      <td>NUM SPACE NOUN NOUN NOUN NOUN NOUN NOUN NOUN ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Poetaster.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>Thy worst .\\nI fart at thee .\\nHa' you your Wi...</td>\n",
       "      <td>PRON ADJ PUNCT SPACE PRON VERB PRON PUNCT SPA...</td>\n",
       "      <td>PRON ADJ PUNCT SPACE PRON VERB PRON PUNCT SPA...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>The Alchemist.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>YOU woful wights , give ear a\\nwhile ,\\nAnd ma...</td>\n",
       "      <td>PRON ADJ NOUN PUNCT X SPACE PUNCT SPACE CCONJ...</td>\n",
       "      <td>PRON ADJ NOUN PUNCT X SPACE PUNCT SPACE CCONJ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>THE CASE IS ALTERED.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>To Earth ?\\nand why to Earth , thou foolish\\nS...</td>\n",
       "      <td>ADP PROPN PUNCT SPACE PROPN PUNCT ADJ SPACE P...</td>\n",
       "      <td>ADP PROPN PUNCT SPACE PROPN PUNCT ADJ SPACE P...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>The Devil is an Ass.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>Welcome , good Captain Ironside , and Bro-\\nth...</td>\n",
       "      <td>INTJ PUNCT ADJ PROPN PROPN PUNCT PROPN SPACE ...</td>\n",
       "      <td>INTJ PUNCT ADJ PROPN PROPN PUNCT PROPN SPACE ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>The Magnetic Lady.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>The SCENE\\nB A R N E T.\\nThe P E R S O N S of ...</td>\n",
       "      <td>DET PROPN SPACE PROPN DET NOUN NOUN NOUN PROP...</td>\n",
       "      <td>DET PROPN SPACE PROPN DET NOUN NOUN NOUN PROP...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>The New Inn.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>he walks in his Gown , Wastcoat , and Trousers...</td>\n",
       "      <td>VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT NUM ...</td>\n",
       "      <td>VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT NUM ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>The Staple of News.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Open the Shrine , that I may see my Saint .\\nH...</td>\n",
       "      <td>VERB PROPN PUNCT PRON VERB PROPN PUNCT SPACE ...</td>\n",
       "      <td>VERB PROPN PUNCT PRON VERB PROPN PUNCT SPACE ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Jonson</td>\n",
       "      <td>Volpone.txt.Jonson.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>\\nCan one self cause , in subjects so alike As...</td>\n",
       "      <td>SPACE VERB PUNCT NOUN ADV ADP PRON NUM VERB P...</td>\n",
       "      <td>SPACE VERB PUNCT NOUN ADV ADP PRON NUM VERB P...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>ALL FOOLS.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>An Humorous Day 's Mirth .\\nLa , Yet hath the ...</td>\n",
       "      <td>DET ADJ PROPN PART PROPN PUNCT SPACE PROPN PU...</td>\n",
       "      <td>DET ADJ PROPN PART PROPN PUNCT SPACE PROPN PU...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>AN HUMOROUS DAYS MIRTH.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>\\nLeave me awhile , my lords , and wait for me...</td>\n",
       "      <td>SPACE VERB ADV PUNCT NOUN PUNCT VERB ADP ADP ...</td>\n",
       "      <td>SPACE VERB ADV PUNCT NOUN PUNCT VERB ADP ADP ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>BEGGAR OF ALEXANDRIA.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>\\nFortune , not Reason , rules the state of th...</td>\n",
       "      <td>SPACE PROPN PUNCT ADV NOUN PUNCT VERB NOUN NO...</td>\n",
       "      <td>SPACE PROPN PUNCT ADV NOUN PUNCT VERB NOUN NO...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>BUSSY DAMBOIS.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>Byron fall 'n in so traitorous a re - lapse , ...</td>\n",
       "      <td>PROPN VERB PUNCT CCONJ ADJ PUNCT NOUN PUNCT V...</td>\n",
       "      <td>PROPN VERB PUNCT CCONJ ADJ PUNCT NOUN PUNCT V...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>BYRONS TRAGEDY.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>Sc?na prima .\\nof the first day . Loose no tim...</td>\n",
       "      <td>NOUN NOUN PUNCT SPACE NOUN PUNCT ADJ NOUN NOU...</td>\n",
       "      <td>NOUN NOUN PUNCT SPACE NOUN PUNCT ADJ NOUN NOU...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>May Day.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>\\nMonsieur D'Olive .\\nPhilip , the Duke .\\nAnn...</td>\n",
       "      <td>SPACE PROPN PROPN PUNCT SPACE PROPN PUNCT PRO...</td>\n",
       "      <td>SPACE PROPN PROPN PUNCT SPACE PROPN PUNCT PRO...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>MONSIEUR DOLIVE.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>\\nTo what will this declining kingdom turn , S...</td>\n",
       "      <td>SPACE ADP VERB NOUN NOUN PUNCT VERB NOUN PUNC...</td>\n",
       "      <td>SPACE ADP VERB NOUN NOUN PUNCT VERB NOUN PUNC...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>REVENGE CF BUSSY DAMBOIS.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>VVhere is Sir Gyles Goosecappe here ?\\nHere my...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PUNCT SPACE ADV PROPN...</td>\n",
       "      <td>PROPN PROPN PROPN PROPN PUNCT SPACE ADV PROPN...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>sir_giles.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>\\nHaste , nephew 1 what , a sluggard ?\\nFie fo...</td>\n",
       "      <td>SPACE PROPN PUNCT NOUN NUM PUNCT ADJ PUNCT SP...</td>\n",
       "      <td>SPACE PROPN PUNCT NOUN NUM PUNCT ADJ PUNCT SP...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>THE GENTLEMAN USHER.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>\\nThou blind imperfect goddess , that delight ...</td>\n",
       "      <td>SPACE PROPN ADJ ADJ NOUN PUNCT NOUN PUNCT INT...</td>\n",
       "      <td>SPACE PROPN ADJ ADJ NOUN PUNCT NOUN PUNCT INT...</td>\n",
       "      <td>5</td>\n",
       "      <td>Chapman</td>\n",
       "      <td>THE WIDOWS TEARS.txt.Chapman.tok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Play_id                                           Raw Text  \\\n",
       "0         0  As I remember , Adam , it was upon this fashio...   \n",
       "1         1  Proceed , Solinus , to procure my fall\\nAnd by...   \n",
       "2         2  Who 's there ?\\nNay , answer me : stand , and ...   \n",
       "3         3  So shaken as we are , so wan with care ,\\nFind...   \n",
       "4         4  Open your ears ; for which of you will stop\\nT...   \n",
       "5         5  O for a Muse of fire , that would ascend\\nThe ...   \n",
       "6         6  Hung be the heavens with black , yield day to ...   \n",
       "7         7  As by your high imperial majesty\\nI had in cha...   \n",
       "8         8  I wonder how the king escaped our hands .\\nWhi...   \n",
       "9         9  Hence ! home , you idle creatures get you home...   \n",
       "10       10  Now , say , Chatillon , what would France with...   \n",
       "11       11  Let fame , that all hunt after in their lives ...   \n",
       "12       12  In sooth , I know not why I am so sad :\\nIt we...   \n",
       "13       13  Sir Hugh , persuade me not ; I will make a Sta...   \n",
       "14       14  Now , fair Hippolyta , our nuptial hour\\nDraws...   \n",
       "15       15  I learn in this letter that Don Peter of Arrag...   \n",
       "16       16  Old John of Gaunt , time-honour 'd Lancaster ,...   \n",
       "17       17  Now is the winter of our discontent\\nMade glor...   \n",
       "18       18  Two households , both alike in dignity ,\\nIn f...   \n",
       "19       19  I 'll pheeze you , in faith .\\nA pair of stock...   \n",
       "20       20  Noble patricians , patrons of my right ,\\nDefe...   \n",
       "21       21  If music be the food of love , play on ;\\nGive...   \n",
       "22       22  Cease to persuade , my loving Proteus :\\nHome-...   \n",
       "23       23  In delivering my son from me , I bury a second...   \n",
       "24       24  Nay , but this dotage of our general 's\\nO'erf...   \n",
       "25       25  Before we proceed any further , hear me speak ...   \n",
       "26       26  You do not meet a man but frowns : our bloods\\...   \n",
       "27       27  I come no more to make you laugh : things now ...   \n",
       "28       28  I thought the king had more affected the Duke ...   \n",
       "29       29  When shall we three meet again\\nIn thunder , l...   \n",
       "..      ...                                                ...   \n",
       "47       47  What Raynulph monk of Chester can\\nRaise from ...   \n",
       "48       48  I am at my wit 's end , Savorwit .\\nAnd I am e...   \n",
       "49       49  My lords ,\\nKnow that we , far from any natura...   \n",
       "50       50  Thus high , my lords , your powers and constan...   \n",
       "51       51  My three years spent in war has now undone\\nMy...   \n",
       "52       52  NOW o ' my Faith , Old Bishop Valen-\\ntine ,\\n...   \n",
       "53       53  such luck to spin out these fine things still ...   \n",
       "54       54  Sylla 's Ghost .\\nDOst thou not feel me , Rome...   \n",
       "55       55  ' Tis I , blind Archer .\\nWho ?\\nMercury ?\\nSt...   \n",
       "56       56  Brain-worm ,\\nCall up your young master : bid ...   \n",
       "57       57  I Would , I could make 'hem a shew my self . I...   \n",
       "58       58  95\\nP O E T A S T E R:\\nO R,\\nHis Arraignment ...   \n",
       "59       59  Thy worst .\\nI fart at thee .\\nHa' you your Wi...   \n",
       "60       60  YOU woful wights , give ear a\\nwhile ,\\nAnd ma...   \n",
       "61       61  To Earth ?\\nand why to Earth , thou foolish\\nS...   \n",
       "62       62  Welcome , good Captain Ironside , and Bro-\\nth...   \n",
       "63       63  The SCENE\\nB A R N E T.\\nThe P E R S O N S of ...   \n",
       "64       64  he walks in his Gown , Wastcoat , and Trousers...   \n",
       "65       65  Open the Shrine , that I may see my Saint .\\nH...   \n",
       "66       66  \\nCan one self cause , in subjects so alike As...   \n",
       "67       67  An Humorous Day 's Mirth .\\nLa , Yet hath the ...   \n",
       "68       68  \\nLeave me awhile , my lords , and wait for me...   \n",
       "69       69  \\nFortune , not Reason , rules the state of th...   \n",
       "70       70  Byron fall 'n in so traitorous a re - lapse , ...   \n",
       "71       71  Sc?na prima .\\nof the first day . Loose no tim...   \n",
       "72       72  \\nMonsieur D'Olive .\\nPhilip , the Duke .\\nAnn...   \n",
       "73       73  \\nTo what will this declining kingdom turn , S...   \n",
       "74       74  VVhere is Sir Gyles Goosecappe here ?\\nHere my...   \n",
       "75       75  \\nHaste , nephew 1 what , a sluggard ?\\nFie fo...   \n",
       "76       76  \\nThou blind imperfect goddess , that delight ...   \n",
       "\n",
       "                                              POS_new  \\\n",
       "0    ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...   \n",
       "1    PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...   \n",
       "2    NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...   \n",
       "3    ADV VERB VERB PUNCT NOUN NOUN PUNCT SPACE VER...   \n",
       "4    VERB NOUN PUNCT ADP ADJ PRON VERB SPACE DET N...   \n",
       "5    INTJ ADP PROPN NOUN PUNCT VERB SPACE DET ADJ ...   \n",
       "6    PROPN NOUN ADJ PUNCT NOUN NOUN NOUN PUNCT SPA...   \n",
       "7    ADP ADP ADJ ADJ NOUN SPACE PRON NOUN NOUN ADP...   \n",
       "8    PRON VERB ADV NOUN VERB ADJ NOUN PUNCT SPACE ...   \n",
       "9    ADV PUNCT PUNCT PRON ADJ NOUN PRON PUNCT SPAC...   \n",
       "10   ADV PUNCT PUNCT PROPN PUNCT PROPN PRON PUNCT ...   \n",
       "11   VERB NOUN PUNCT NOUN ADP ADJ NOUN PUNCT SPACE...   \n",
       "12   ADP NOUN PUNCT PRON VERB ADV PRON VERB ADJ PU...   \n",
       "13   PROPN PROPN PUNCT VERB ADV PUNCT PRON ADJ SPA...   \n",
       "14   ADV PUNCT ADJ PROPN PUNCT ADJ ADJ NOUN SPACE ...   \n",
       "15   PRON VERB NOUN PROPN PROPN PROPN SPACE NOUN P...   \n",
       "16   PROPN PROPN PROPN PUNCT NOUN PUNCT NOUN PROPN...   \n",
       "17   ADV NOUN ADJ NOUN SPACE VERB ADJ NOUN ADP NOU...   \n",
       "18   NUM NOUN PUNCT ADV NOUN PUNCT SPACE ADP ADJ P...   \n",
       "19   PRON VERB PRON PUNCT NOUN PUNCT SPACE DET NOU...   \n",
       "20   ADJ NOUN PUNCT NOUN PUNCT SPACE VERB NOUN NOU...   \n",
       "21   ADP NOUN NOUN NOUN PUNCT VERB PART PUNCT SPAC...   \n",
       "22   VERB VERB PUNCT VERB PROPN PUNCT SPACE NOUN P...   \n",
       "23   ADP VERB NOUN PUNCT PRON VERB ADJ NOUN PUNCT ...   \n",
       "24   INTJ PUNCT NOUN ADJ NOUN PART SPACE VERB NOUN...   \n",
       "25   ADP VERB DET PUNCT VERB VERB PUNCT SPACE PROP...   \n",
       "26   PRON VERB ADV VERB NOUN VERB PUNCT ADJ NOUN S...   \n",
       "27   PRON ADJ PRON VERB PUNCT NOUN PUNCT SPACE ADJ...   \n",
       "28   PRON VERB NOUN ADV PROPN SPACE PROPN PROPN PU...   \n",
       "29   ADV VERB VERB SPACE ADP NOUN PUNCT NOUN PUNCT...   \n",
       "..                                                ...   \n",
       "47   NOUN PROPN NOUN PROPN SPACE PROPN SPACE ADJ V...   \n",
       "48   PRON VERB NOUN PART PUNCT PROPN PUNCT SPACE C...   \n",
       "49   ADJ NOUN PUNCT SPACE VERB PUNCT ADV DET ADJ N...   \n",
       "50   ADV ADJ PUNCT NOUN PUNCT NOUN ADJ NOUN SPACE ...   \n",
       "51   ADJ NOUN VERB NOUN VERB SPACE ADJ NOUN ADV PU...   \n",
       "52   ADV PUNCT NOUN PUNCT PROPN PROPN PROPN SPACE ...   \n",
       "53   ADJ NOUN VERB PART DET ADJ NOUN ADV PUNCT SPA...   \n",
       "54   PROPN PART PROPN PUNCT SPACE PROPN ADV VERB P...   \n",
       "55   PUNCT PROPN PRON PUNCT ADJ PROPN PUNCT SPACE ...   \n",
       "56   NOUN PUNCT NOUN PUNCT SPACE VERB ADJ NOUN PUN...   \n",
       "57   PRON VERB PUNCT PRON VERB PUNCT VERB NOUN PUN...   \n",
       "58   NUM SPACE NOUN NOUN NOUN NOUN NOUN NOUN NOUN ...   \n",
       "59   PRON ADJ PUNCT SPACE PRON VERB PRON PUNCT SPA...   \n",
       "60   PRON ADJ NOUN PUNCT X SPACE PUNCT SPACE CCONJ...   \n",
       "61   ADP PROPN PUNCT SPACE PROPN PUNCT ADJ SPACE P...   \n",
       "62   INTJ PUNCT ADJ PROPN PROPN PUNCT PROPN SPACE ...   \n",
       "63   DET PROPN SPACE PROPN DET NOUN NOUN NOUN PROP...   \n",
       "64   VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT NUM ...   \n",
       "65   VERB PROPN PUNCT PRON VERB PROPN PUNCT SPACE ...   \n",
       "66   SPACE VERB PUNCT NOUN ADV ADP PRON NUM VERB P...   \n",
       "67   DET ADJ PROPN PART PROPN PUNCT SPACE PROPN PU...   \n",
       "68   SPACE VERB ADV PUNCT NOUN PUNCT VERB ADP ADP ...   \n",
       "69   SPACE PROPN PUNCT ADV NOUN PUNCT VERB NOUN NO...   \n",
       "70   PROPN VERB PUNCT CCONJ ADJ PUNCT NOUN PUNCT V...   \n",
       "71   NOUN NOUN PUNCT SPACE NOUN PUNCT ADJ NOUN NOU...   \n",
       "72   SPACE PROPN PROPN PUNCT SPACE PROPN PUNCT PRO...   \n",
       "73   SPACE ADP VERB NOUN NOUN PUNCT VERB NOUN PUNC...   \n",
       "74   PROPN PROPN PROPN PROPN PUNCT SPACE ADV PROPN...   \n",
       "75   SPACE PROPN PUNCT NOUN NUM PUNCT ADJ PUNCT SP...   \n",
       "76   SPACE PROPN ADJ ADJ NOUN PUNCT NOUN PUNCT INT...   \n",
       "\n",
       "                                         POSstops_new  Label         Author  \\\n",
       "0    ADP PRON VERB PUNCT PROPN PUNCT NOUN SPACE VE...      0  E-Shakespeare   \n",
       "1    PROPN PUNCT PROPN PUNCT VERB NOUN SPACE CCONJ...      0  E-Shakespeare   \n",
       "2    NOUN VERB PUNCT SPACE PROPN PUNCT VERB PUNCT ...      0  E-Shakespeare   \n",
       "3    ADV VERB VERB PUNCT NOUN NOUN PUNCT SPACE VER...      0  E-Shakespeare   \n",
       "4    VERB NOUN PUNCT ADP ADJ PRON VERB SPACE DET N...      0  E-Shakespeare   \n",
       "5    INTJ ADP PROPN NOUN PUNCT VERB SPACE DET ADJ ...      0  E-Shakespeare   \n",
       "6    PROPN NOUN ADJ PUNCT NOUN NOUN NOUN PUNCT SPA...      0  E-Shakespeare   \n",
       "7    ADP ADP ADJ ADJ NOUN SPACE PRON NOUN NOUN ADP...      0  E-Shakespeare   \n",
       "8    PRON VERB ADV NOUN VERB ADJ NOUN PUNCT SPACE ...      0  E-Shakespeare   \n",
       "9    ADV PUNCT PUNCT PRON ADJ NOUN PRON PUNCT SPAC...      0  E-Shakespeare   \n",
       "10   ADV PUNCT PUNCT PROPN PUNCT PROPN PRON PUNCT ...      0  E-Shakespeare   \n",
       "11   VERB NOUN PUNCT NOUN ADP ADJ NOUN PUNCT SPACE...      0  E-Shakespeare   \n",
       "12   ADP NOUN PUNCT PRON VERB ADV PRON VERB ADJ PU...      0  E-Shakespeare   \n",
       "13   PROPN PROPN PUNCT VERB ADV PUNCT PRON ADJ SPA...      0  E-Shakespeare   \n",
       "14   ADV PUNCT ADJ PROPN PUNCT ADJ ADJ NOUN SPACE ...      0  E-Shakespeare   \n",
       "15   PRON VERB NOUN PROPN PROPN PROPN SPACE NOUN P...      0  E-Shakespeare   \n",
       "16   PROPN PROPN PROPN PUNCT NOUN PUNCT NOUN PROPN...      0  E-Shakespeare   \n",
       "17   ADV NOUN ADJ NOUN SPACE VERB ADJ NOUN ADP NOU...      0  E-Shakespeare   \n",
       "18   NUM NOUN PUNCT ADV NOUN PUNCT SPACE ADP ADJ P...      0  E-Shakespeare   \n",
       "19   PRON VERB PRON PUNCT NOUN PUNCT SPACE DET NOU...      0  E-Shakespeare   \n",
       "20   ADJ NOUN PUNCT NOUN PUNCT SPACE VERB NOUN NOU...      0  E-Shakespeare   \n",
       "21   ADP NOUN NOUN NOUN PUNCT VERB PART PUNCT SPAC...      0  E-Shakespeare   \n",
       "22   VERB VERB PUNCT VERB PROPN PUNCT SPACE NOUN P...      0  E-Shakespeare   \n",
       "23   ADP VERB NOUN PUNCT PRON VERB ADJ NOUN PUNCT ...      1  L-Shakespeare   \n",
       "24   INTJ PUNCT NOUN ADJ NOUN PART SPACE VERB NOUN...      1  L-Shakespeare   \n",
       "25   ADP VERB DET PUNCT VERB VERB PUNCT SPACE PROP...      1  L-Shakespeare   \n",
       "26   PRON VERB ADV VERB NOUN VERB PUNCT ADJ NOUN S...      1  L-Shakespeare   \n",
       "27   PRON ADJ PRON VERB PUNCT NOUN PUNCT SPACE ADJ...      1  L-Shakespeare   \n",
       "28   PRON VERB NOUN ADV PROPN SPACE PROPN PROPN PU...      1  L-Shakespeare   \n",
       "29   ADV VERB VERB SPACE ADP NOUN PUNCT NOUN PUNCT...      1  L-Shakespeare   \n",
       "..                                                ...    ...            ...   \n",
       "47   NOUN PROPN NOUN PROPN SPACE PROPN SPACE ADJ V...      3      Middleton   \n",
       "48   PRON VERB NOUN PART PUNCT PROPN PUNCT SPACE C...      3      Middleton   \n",
       "49   ADJ NOUN PUNCT SPACE VERB PUNCT ADV DET ADJ N...      3      Middleton   \n",
       "50   ADV ADJ PUNCT NOUN PUNCT NOUN ADJ NOUN SPACE ...      3      Middleton   \n",
       "51   ADJ NOUN VERB NOUN VERB SPACE ADJ NOUN ADV PU...      3      Middleton   \n",
       "52   ADV PUNCT NOUN PUNCT PROPN PROPN PROPN SPACE ...      4         Jonson   \n",
       "53   ADJ NOUN VERB PART DET ADJ NOUN ADV PUNCT SPA...      4         Jonson   \n",
       "54   PROPN PART PROPN PUNCT SPACE PROPN ADV VERB P...      4         Jonson   \n",
       "55   PUNCT PROPN PRON PUNCT ADJ PROPN PUNCT SPACE ...      4         Jonson   \n",
       "56   NOUN PUNCT NOUN PUNCT SPACE VERB ADJ NOUN PUN...      4         Jonson   \n",
       "57   PRON VERB PUNCT PRON VERB PUNCT VERB NOUN PUN...      4         Jonson   \n",
       "58   NUM SPACE NOUN NOUN NOUN NOUN NOUN NOUN NOUN ...      4         Jonson   \n",
       "59   PRON ADJ PUNCT SPACE PRON VERB PRON PUNCT SPA...      4         Jonson   \n",
       "60   PRON ADJ NOUN PUNCT X SPACE PUNCT SPACE CCONJ...      4         Jonson   \n",
       "61   ADP PROPN PUNCT SPACE PROPN PUNCT ADJ SPACE P...      4         Jonson   \n",
       "62   INTJ PUNCT ADJ PROPN PROPN PUNCT PROPN SPACE ...      4         Jonson   \n",
       "63   DET PROPN SPACE PROPN DET NOUN NOUN NOUN PROP...      4         Jonson   \n",
       "64   VERB PROPN PUNCT PROPN PUNCT PROPN PUNCT NUM ...      4         Jonson   \n",
       "65   VERB PROPN PUNCT PRON VERB PROPN PUNCT SPACE ...      4         Jonson   \n",
       "66   SPACE VERB PUNCT NOUN ADV ADP PRON NUM VERB P...      5        Chapman   \n",
       "67   DET ADJ PROPN PART PROPN PUNCT SPACE PROPN PU...      5        Chapman   \n",
       "68   SPACE VERB ADV PUNCT NOUN PUNCT VERB ADP ADP ...      5        Chapman   \n",
       "69   SPACE PROPN PUNCT ADV NOUN PUNCT VERB NOUN NO...      5        Chapman   \n",
       "70   PROPN VERB PUNCT CCONJ ADJ PUNCT NOUN PUNCT V...      5        Chapman   \n",
       "71   NOUN NOUN PUNCT SPACE NOUN PUNCT ADJ NOUN NOU...      5        Chapman   \n",
       "72   SPACE PROPN PROPN PUNCT SPACE PROPN PUNCT PRO...      5        Chapman   \n",
       "73   SPACE ADP VERB NOUN NOUN PUNCT VERB NOUN PUNC...      5        Chapman   \n",
       "74   PROPN PROPN PROPN PROPN PUNCT SPACE ADV PROPN...      5        Chapman   \n",
       "75   SPACE PROPN PUNCT NOUN NUM PUNCT ADJ PUNCT SP...      5        Chapman   \n",
       "76   SPACE PROPN ADJ ADJ NOUN PUNCT NOUN PUNCT INT...      5        Chapman   \n",
       "\n",
       "                                        Play  \n",
       "0          asyoulikeit.txt.E-Shakespeare.tok  \n",
       "1        comedy_errors.txt.E-Shakespeare.tok  \n",
       "2               Hamlet.txt.E-Shakespeare.tok  \n",
       "3         henryivPart1.txt.E-Shakespeare.tok  \n",
       "4         henryivPart2.txt.E-Shakespeare.tok  \n",
       "5               henryv.txt.E-Shakespeare.tok  \n",
       "6         henryviPart1.txt.E-Shakespeare.tok  \n",
       "7         henryviPart2.txt.E-Shakespeare.tok  \n",
       "8         henryviPart3.txt.E-Shakespeare.tok  \n",
       "9        julius_caesar.txt.E-Shakespeare.tok  \n",
       "10           King John.txt.E-Shakespeare.tok  \n",
       "11   Love_Labours_lost.txt.E-Shakespeare.tok  \n",
       "12     merchant-venice.txt.E-Shakespeare.tok  \n",
       "13         merry_wives.txt.E-Shakespeare.tok  \n",
       "14           midsummer.txt.E-Shakespeare.tok  \n",
       "15            much_ado.txt.E-Shakespeare.tok  \n",
       "16           richardii.txt.E-Shakespeare.tok  \n",
       "17          richardiii.txt.E-Shakespeare.tok  \n",
       "18        romeo_juliet.txt.E-Shakespeare.tok  \n",
       "19        taming_shrew.txt.E-Shakespeare.tok  \n",
       "20    Titus_Andronicus.txt.E-Shakespeare.tok  \n",
       "21       twelfth_night.txt.E-Shakespeare.tok  \n",
       "22       two_gentlemen.txt.E-Shakespeare.tok  \n",
       "23            allswell.txt.L-Shakespeare.tok  \n",
       "24           Cleopatra.txt.L-Shakespeare.tok  \n",
       "25          coriolanus.txt.L-Shakespeare.tok  \n",
       "26           Cymbeline.txt.L-Shakespeare.tok  \n",
       "27           henryviii.txt.L-Shakespeare.tok  \n",
       "28                Lear.txt.L-Shakespeare.tok  \n",
       "29             Macbeth.txt.L-Shakespeare.tok  \n",
       "..                                       ...  \n",
       "47                 hengist.txt.Middleton.tok  \n",
       "48                  no wit.txt.Middleton.tok  \n",
       "49                 Phoenix.txt.Middleton.tok  \n",
       "50       the second maiden.txt.Middleton.tok  \n",
       "51               the witch.txt.Middleton.tok  \n",
       "52            A Tale of a Tub.txt.Jonson.tok  \n",
       "53            Bartholmew Fair.txt.Jonson.tok  \n",
       "54    Catiline His Conspiracy.txt.Jonson.tok  \n",
       "55            Cynthias Revels.txt.Jonson.tok  \n",
       "56    Every Man in his Humour.txt.Jonson.tok  \n",
       "57              Love_Restored.txt.Jonson.tok  \n",
       "58                  Poetaster.txt.Jonson.tok  \n",
       "59              The Alchemist.txt.Jonson.tok  \n",
       "60        THE CASE IS ALTERED.txt.Jonson.tok  \n",
       "61        The Devil is an Ass.txt.Jonson.tok  \n",
       "62          The Magnetic Lady.txt.Jonson.tok  \n",
       "63                The New Inn.txt.Jonson.tok  \n",
       "64         The Staple of News.txt.Jonson.tok  \n",
       "65                    Volpone.txt.Jonson.tok  \n",
       "66                 ALL FOOLS.txt.Chapman.tok  \n",
       "67    AN HUMOROUS DAYS MIRTH.txt.Chapman.tok  \n",
       "68      BEGGAR OF ALEXANDRIA.txt.Chapman.tok  \n",
       "69             BUSSY DAMBOIS.txt.Chapman.tok  \n",
       "70            BYRONS TRAGEDY.txt.Chapman.tok  \n",
       "71                   May Day.txt.Chapman.tok  \n",
       "72           MONSIEUR DOLIVE.txt.Chapman.tok  \n",
       "73  REVENGE CF BUSSY DAMBOIS.txt.Chapman.tok  \n",
       "74                 sir_giles.txt.Chapman.tok  \n",
       "75       THE GENTLEMAN USHER.txt.Chapman.tok  \n",
       "76          THE WIDOWS TEARS.txt.Chapman.tok  \n",
       "\n",
       "[77 rows x 7 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:20:22.163845Z",
     "start_time": "2019-02-24T19:20:20.944774Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imp.to_excel(\"DataFrame_imp.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:16:56.421468Z",
     "start_time": "2019-02-24T19:16:56.408130Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_features_GM_imp(X_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: data used to get model, bot for testing and training\n",
    "    output: features for train and test instances \n",
    "    \"\"\"\n",
    "    \n",
    "    #getting the columns I wanna use for my features from the instances passed\n",
    "    X_txt = X_train[\"Raw Text\"]\n",
    "    X_POS = X_train[\"POS_new\"]\n",
    "    X_POSstops = X_train[ \"POSstops_new\"]\n",
    "    \n",
    "    X_test_txt= X_test[\"Raw Text\"]\n",
    "    X_test_POS = X_test[\"POS_new\"]\n",
    "    X_test_POSstops = X_test[ \"POSstops_new\"]\n",
    "    \n",
    "    \n",
    "    # using countvectorizors to get freqs \n",
    "    cvec1 = CountVectorizer(vocabulary= stopwords_747, max_features=1000, strip_accents=\"ascii\")#word freq of stop words\n",
    "    cvec2 = CountVectorizer(max_features=1000, strip_accents=\"ascii\") #freq of POS taggs\n",
    "    cvec3 = CountVectorizer(ngram_range=(2,2),max_features=1000, strip_accents=\"ascii\") #ngrams of POS/stops \n",
    "\n",
    "   # fitting  and transforming of train data, and stack vectors at the same time to get X for model, \n",
    "    train_features= np.hstack((\n",
    "        cvec1.fit_transform(X_txt).toarray(),\n",
    "        cvec2.fit_transform(X_POS).toarray(),\n",
    "        cvec3.fit_transform(X_POSstops).toarray(),\n",
    "        ))\n",
    "    \n",
    "    #only fit X_test data\n",
    "    test_features = np.hstack((\n",
    "            cvec1.transform(X_test_txt).toarray(),\n",
    "            cvec2.transform(X_test_POS).toarray(),\n",
    "            cvec3.transform(X_test_POSstops).toarray(),\n",
    "        ))\n",
    "\n",
    "    \n",
    "    return train_features, test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:22:48.605862Z",
     "start_time": "2019-02-24T19:22:48.594056Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_imp(dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    change function get_features_GM(train_data, test_data) for feature engineering!\n",
    "    input: Dataframe\n",
    "    \n",
    "    output: predictions for test instances\n",
    "    \"\"\"\n",
    "    \n",
    "    instances= get_LOO_instances(dataframe)\n",
    "    predictions_NB=[]\n",
    "    predictions_SVM=[]\n",
    "    for inst in instances:\n",
    "        train_data= inst[0]\n",
    "        test_data = inst[1]\n",
    "        train_label=train_data[\"Label\"]\n",
    "        train_features, test_features= get_features_GM_imp(train_data, test_data)\n",
    "        pred_NB, coeff_NB, cls_NB = predict_NB(train_features, train_label, test_features)\n",
    "        pred_SVM, coeff_SVM, cls_SVM = predict_SVM(train_features, train_label, test_features)\n",
    "        predictions_NB.append(pred_NB)\n",
    "        predictions_SVM.append(pred_SVM)\n",
    "        \n",
    "        \n",
    "    return np.array(predictions_NB), coeff_NB, cls_NB, np.array(predictions_SVM), coeff_SVM, cls_SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:30:54.560750Z",
     "start_time": "2019-02-24T19:22:50.185502Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_NB_imp, coeff_NB_imp, cls_NB_imp, predictions_SVM_imp, coeff_SVM_imp, cls_SVM_imp  =  get_preds_imp(df_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:30:54.575529Z",
     "start_time": "2019-02-24T19:30:54.565843Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Pred_NB_imp\"]=predictions_NB_imp #I add my predictions to the data frame as a new column\n",
    "df[\"Pred_SVM_imp\"]=predictions_SVM_imp #I add my predictions to the data frame as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:30:54.594895Z",
     "start_time": "2019-02-24T19:30:54.579515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_NB_imp\"] =df[\"Label\"]==df[ \"Pred_NB_imp\"]\n",
    "acc_NB_imp = df.loc[df.acc_NB_imp == True, 'acc_NB_imp'].count()/df.shape[0]\n",
    "print(acc_NB_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:30:54.605932Z",
     "start_time": "2019-02-24T19:30:54.597352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441558441558441\n"
     ]
    }
   ],
   "source": [
    "#get the acc by comparing pred to the labels \n",
    "df[\"acc_SVM_imp\"] =df[\"Label\"]==df[ \"Pred_SVM_imp\"]\n",
    "acc_SVM_imp = df.loc[df.acc_SVM_imp == True, 'acc_SVM_imp'].count()/df.shape[0]\n",
    "print(acc_SVM_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:57:23.593985Z",
     "start_time": "2019-02-24T19:57:23.586533Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_NB = np.append(predictions_NB_imp, acc_NB_imp)\n",
    "imp_SVM = np.append(predictions_SVM_imp, acc_SVM_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T19:57:23.987764Z",
     "start_time": "2019-02-24T19:57:23.979956Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stpw_results[\"Pred_NB_imp\"] =imp_NB\n",
    "df_stpw_results[\"Pred_SVM_imp\"] =imp_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T20:00:13.736589Z",
     "start_time": "2019-02-24T20:00:13.670239Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stpw_results\n",
    "df_stpw_results.to_excel(\"Stopword_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results__\n",
    "\n",
    "The highest results found with 507 stopwords using a multi SVM\n",
    "\n",
    "\n",
    "| n_stop_words \t| NB \t| SVM \t|  \n",
    "|--------------\t|--------\t|--------\t|\n",
    "| *305*  \t| 0.7792 \t| 0.8052 \t| \n",
    "| *507*  | 0.7792    | __0.8442__ |\n",
    "| *710*\t| 0.7792 \t| 0.8052 \t|\n",
    "| *747*\t| 0.7792 \t| 0.7792 \t| \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
